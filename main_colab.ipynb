{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14006,
     "status": "ok",
     "timestamp": 1565220643458,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "aOhZRpq_LZzs",
    "outputId": "c7df4200-b8d7-4aac-9fbd-ccb0ab8cd0c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopandas\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/80/da2a33c9201cd4ce693f4aa6189efc9ef1a48bec1c3b02c3ce9908b07fec/geopandas-0.5.1-py2.py3-none-any.whl (893kB)\n",
      "\u001b[K     |████████████████████████████████| 901kB 9.6MB/s \n",
      "\u001b[?25hCollecting fiona (from geopandas)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/4a/193cd6a75e51062c85f4e1cd6f312b3bbda6e26ba7510f152ef5016f0b16/Fiona-1.8.6-cp36-cp36m-manylinux1_x86_64.whl (17.9MB)\n",
      "\u001b[K     |████████████████████████████████| 17.9MB 42.3MB/s \n",
      "\u001b[?25hCollecting pyproj (from geopandas)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/59/43869adef45ce4f1cf7d5c3aef1ea5d65d449050abdda5de7a2465c5729d/pyproj-2.2.1-cp36-cp36m-manylinux1_x86_64.whl (11.2MB)\n",
      "\u001b[K     |████████████████████████████████| 11.2MB 39.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from geopandas) (0.24.2)\n",
      "Requirement already satisfied: shapely in /usr/local/lib/python3.6/dist-packages (from geopandas) (1.6.4.post2)\n",
      "Collecting click-plugins>=1.0 (from fiona->geopandas)\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/da/824b92d9942f4e472702488857914bdd50f73021efea15b4cad9aca8ecef/click_plugins-1.1.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (19.1.0)\n",
      "Collecting cligj>=0.5 (from fiona->geopandas)\n",
      "  Downloading https://files.pythonhosted.org/packages/e4/be/30a58b4b0733850280d01f8bd132591b4668ed5c7046761098d665ac2174/cligj-0.5.0-py3-none-any.whl\n",
      "Collecting munch (from fiona->geopandas)\n",
      "  Downloading https://files.pythonhosted.org/packages/68/f4/260ec98ea840757a0da09e0ed8135333d59b8dfebe9752a365b04857660a/munch-2.3.2.tar.gz\n",
      "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (1.12.0)\n",
      "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (7.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pandas->geopandas) (1.16.4)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->geopandas) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->geopandas) (2.5.3)\n",
      "Building wheels for collected packages: munch\n",
      "  Building wheel for munch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for munch: filename=munch-2.3.2-py2.py3-none-any.whl size=6613 sha256=1add48b354cb1ce642e85b67711202a9133bce7e4718627bf662b3b191866442\n",
      "  Stored in directory: /root/.cache/pip/wheels/db/bf/bc/06a3e1bfe0ab27d2e720ceb3cff3159398d92644c0cec2c125\n",
      "Successfully built munch\n",
      "Installing collected packages: click-plugins, cligj, munch, fiona, pyproj, geopandas\n",
      "Successfully installed click-plugins-1.1.1 cligj-0.5.0 fiona-1.8.6 geopandas-0.5.1 munch-2.3.2 pyproj-2.2.1\n",
      "Collecting netCDF4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/94/eacbf790ceb237bee5db9c54e7c225ff0898d6dd496166a21d7c57254eb5/netCDF4-1.5.1.2-cp36-cp36m-manylinux1_x86_64.whl (4.1MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1MB 9.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from netCDF4) (1.16.4)\n",
      "Collecting cftime (from netCDF4)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/64/8ceadda42af3c1b27ee77005807e38c6d77baef28a8f9216b60577fddd71/cftime-1.0.3.4-cp36-cp36m-manylinux1_x86_64.whl (305kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 60.1MB/s \n",
      "\u001b[?25hInstalling collected packages: cftime, netCDF4\n",
      "Successfully installed cftime-1.0.3.4 netCDF4-1.5.1.2\n"
     ]
    }
   ],
   "source": [
    "#download required dependencies \n",
    "!pip install geopandas\n",
    "!pip install netCDF4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25373,
     "status": "ok",
     "timestamp": 1565220671481,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "IgBl6yb1UH06",
    "outputId": "5bcf5d3b-308b-4108-d9aa-9f49e2cec85b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#mount drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v41YJZT9Ga8m"
   },
   "outputs": [],
   "source": [
    "#load in functions required and dependencies to use them\n",
    "%run all_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V3EUspTTGa8s"
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "\n",
    "colabDataPath = '/content/'\n",
    "\n",
    "pickle_in = open('flowDict.pickle','rb')\n",
    "flowDict = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open('tempDict_AB.pickle','rb')\n",
    "tempDict = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open('precDict_AB.pickle','rb')\n",
    "precDict = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open('relHDict_AB.pickle','rb')\n",
    "relHDict = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open('ssrdDict_AB.pickle','rb')\n",
    "ssrdDict = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "je2Z5lksGa8v"
   },
   "outputs": [],
   "source": [
    "#unpack data\n",
    "\n",
    "stationLat = flowDict['stationLat']\n",
    "stationLon = flowDict['stationLon']\n",
    "eraLat = tempDict['latERA']\n",
    "eraLon = tempDict['lonERA']\n",
    "\n",
    "flowDays = flowDict['windowDays']\n",
    "flowMonths = flowDict['windowMonths']\n",
    "flowYears = flowDict['windowYears']\n",
    "eraDays = tempDict['daysERA']\n",
    "eraMonths = tempDict['monthsERA']\n",
    "eraYears = tempDict['yearsERA']\n",
    "\n",
    "F = flowDict['all_flowwindow_norm_NF'] #normalized discharge with nans filled (NF)\n",
    "T = tempDict['T']\n",
    "P = precDict['P']\n",
    "H = relHDict['H']\n",
    "S = ssrdDict['S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set nan values to mean values of field (ie: out of province values\n",
    "\n",
    "meanT = np.nanmean(T)\n",
    "meanP = np.nanmean(P)\n",
    "meanH = np.nanmean(H)\n",
    "meanS = np.nanmean(S)\n",
    "\n",
    "Tall = np.copy(T)\n",
    "Tall[np.where(np.isnan(Tall))] = np.nanmean(T)\n",
    "T = Tall\n",
    "\n",
    "Pall = np.copy(P)\n",
    "Pall[np.where(np.isnan(Pall))] = np.nanmean(P)\n",
    "P = Pall\n",
    "\n",
    "Hall = np.copy(H)\n",
    "Hall[np.where(np.isnan(Hall))] = np.nanmean(H)\n",
    "H = Hall\n",
    "\n",
    "Sall = np.copy(S)\n",
    "Sall[np.where(np.isnan(Sall))] = np.nanmean(S)\n",
    "S = Sall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fs4RbW38Ga8y"
   },
   "outputs": [],
   "source": [
    "#make data have same time range\n",
    "startYear = max(int(np.min(eraYears)),int(np.min(flowYears)))\n",
    "\n",
    "indStartERA = min(np.argwhere(eraYears==startYear))[0]\n",
    "indStartFlow = min(np.argwhere(flowYears==startYear))[0]\n",
    "\n",
    "F = np.asarray(np.transpose(np.squeeze(F[indStartFlow:])))\n",
    "T = np.asarray(T[indStartERA:])\n",
    "P = np.asarray(P[indStartERA:])\n",
    "H = np.asarray(H[indStartERA:])\n",
    "S = np.asarray(S[indStartERA:])\n",
    "\n",
    "#just alberta\n",
    "T = T[:,:15,29:]\n",
    "P = P[:,:15,29:]\n",
    "H = H[:,:15,28:]\n",
    "S = S[:,:15,28:]\n",
    "\n",
    "flowDays = flowDays[indStartFlow:]\n",
    "flowMonths = flowMonths[indStartFlow:]\n",
    "flowYears = flowYears[indStartFlow:]\n",
    "\n",
    "eraDays = eraDays[indStartERA:]\n",
    "eraMonths = eraMonths[indStartERA:]\n",
    "eraYears = eraYears[indStartERA:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(F),np.shape(T),np.shape(P),np.shape(H),np.shape(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep data: standardize\n",
    "\n",
    "#indices of testing/training\n",
    "trainStartYear = 1987\n",
    "trainFinYear = 2000\n",
    "testStartYear = 2001\n",
    "testFinYear = 2010\n",
    "\n",
    "trainInds = np.squeeze(np.argwhere((flowYears>=trainStartYear) & (flowYears<=trainFinYear)))\n",
    "testInds = np.squeeze(np.argwhere((flowYears>=testStartYear) & (flowYears<=testFinYear)))\n",
    "\n",
    "#standardize variables individually (normalize wrt training period), then save as 32-bit rather than 64-bit for space\n",
    "Tmean_train = np.mean([T[trainInds[ii]] for ii in range(len(trainInds))])\n",
    "Tstd_train = np.std([T[trainInds[ii]] for ii in range(len(trainInds))])\n",
    "Tnorm = (T - Tmean_train)/Tstd_train\n",
    "Tnorm = np.single(Tnorm)\n",
    "\n",
    "Pmean_train = np.mean([P[trainInds[ii]] for ii in range(len(trainInds))])\n",
    "Pstd_train = np.std([P[trainInds[ii]] for ii in range(len(trainInds))])\n",
    "Pnorm = (P - Pmean_train)/Pstd_train\n",
    "Pnorm = np.single(Pnorm)\n",
    "\n",
    "Hmean_train = np.mean([H[trainInds[ii]] for ii in range(len(trainInds))])\n",
    "Hstd_train = np.std([H[trainInds[ii]] for ii in range(len(trainInds))])\n",
    "Hnorm = (H - Hmean_train)/Hstd_train\n",
    "Hnorm = np.single(Hnorm)\n",
    "\n",
    "Smean_train = np.mean([S[trainInds[ii]] for ii in range(len(trainInds))])\n",
    "Sstd_train = np.std([S[trainInds[ii]] for ii in range(len(trainInds))])\n",
    "Snorm = (S - Smean_train)/Sstd_train\n",
    "Snorm = np.single(Snorm)\n",
    "\n",
    "#Fmean_train = np.nanmean([F[ii][trainInds[366:]] for ii in range(len(F))])\n",
    "#Fstd_train = np.nanstd([F[ii][trainInds[366:]] for ii in range(len(F))])\n",
    "Fmean_train = np.mean(F[trainInds[366:],:])\n",
    "Fstd_train = np.std(F[trainInds[366:],:])\n",
    "Fnorm = (F - Fmean_train)/Fstd_train\n",
    "Fnorm = np.single(Fnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct train and test predictor/target tensors\n",
    "\n",
    "#target data\n",
    "y_train = np.squeeze([Fnorm[366:len(trainInds)+366,ii] for ii in range(np.shape(F)[1])])\n",
    "y_test = np.squeeze([Fnorm[testInds[1:],ii] for ii in range(np.shape(F)[1])])\n",
    "\n",
    "#first, make (n_time x n_lon x n_lat x n_vars) tensor \n",
    "#x_intermediate = np.zeros((8766,17,43,2))\n",
    "x_intermediate = np.empty((8766,15,14,4),dtype='single')\n",
    "x_intermediate[:,:,:,0] = Tnorm\n",
    "x_intermediate[:,:,:,1] = Pnorm\n",
    "x_intermediate[:,:,:,2] = Hnorm\n",
    "x_intermediate[:,:,:,3] = Snorm\n",
    "x_train_intermediate = x_intermediate[trainInds]\n",
    "x_test_intermediate = x_intermediate[testInds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.shape(x_test), np.shape(y_test), np.shape(x_train), np.shape(y_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d43LmpUmGa83"
   },
   "outputs": [],
   "source": [
    "#now, convert x_intermediate into (n_time x 365 x n_lon x n_lat x n_vars) tensor\n",
    "x = np.empty((8766-365,365,15,14,4),dtype='single')\n",
    "x_train = np.empty((len(trainInds),365,15,14,4),dtype='single')\n",
    "x_test = np.empty((len(testInds)-1,365,15,14,4),dtype='single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JEECQyReGa86"
   },
   "outputs": [],
   "source": [
    "for ii in range(1000):\n",
    "    x_train[ii] = x_intermediate[ii:ii+365]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 972,
     "status": "ok",
     "timestamp": 1565221242897,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "vC7SsmlDGa89",
    "outputId": "8c7a9dc7-128d-4416-fd15-e1e18d20340b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1826, 1461, 364, 1825]"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ii in range(1000,2000):\n",
    "    x_train[ii] = x_intermediate[ii:ii+365]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(2000,3000):\n",
    "    x_train[ii] = x_intermediate[ii:ii+365]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(3000,4000):\n",
    "    x_train[ii] = x_intermediate[ii:ii+365]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(4000,len(trainInds)):\n",
    "    x_train[ii] = x_intermediate[ii:ii+365]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(1000):\n",
    "    x_test[ii] = x_intermediate[ii+len(trainInds)-365:ii+len(trainInds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(1000,2000):\n",
    "    x_test[ii] = x_intermediate[ii+len(trainInds)-365:ii+len(trainInds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(2000,len(testInds)-1):\n",
    "    x_test[ii] = x_intermediate[ii+len(trainInds)-365:ii+len(trainInds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveTrainVars=0\n",
    "if saveTrainVars:\n",
    "    np.savez(\"T_P_H_train_1987_2000_era_interim\", x_train)\n",
    "    #np.savez_compressed(\"T_P_train_1987_2000_era_interim_compressed\", x_train)\n",
    "    \n",
    "saveTestVars=0\n",
    "if saveTestVars:\n",
    "    np.savez(\"T_P_H_test_1987_2000_era_interim\", x_test)\n",
    "    #np.savez_compressed(\"T_P_test_1987_2000_era_interim_compressed\", x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2893,
     "status": "ok",
     "timestamp": 1565221307851,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "qLLvg7nHGa9Q",
    "outputId": "76d94c5f-bb7c-4622-b7f8-192997f135f6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0807 23:41:46.893978 140006467065728 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0807 23:41:46.930340 140006467065728 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0807 23:41:46.937351 140006467065728 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0807 23:41:46.972561 140006467065728 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0807 23:41:47.212259 140006467065728 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import Model, Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, TimeDistributed, LSTM, Dense\n",
    "\n",
    "#CNN model\n",
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(\n",
    "    Conv2D(filters = 8, kernel_size = (3,3), activation='relu',data_format='channels_last', padding='same'), \n",
    "    input_shape=(365,17,43,3)))\n",
    "model.add(TimeDistributed(\n",
    "    Conv2D(filters = 8, kernel_size = (3,3), activation='relu',data_format='channels_last', padding='same'), \n",
    "    input_shape=(365,17,43,3)))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size = 2)))\n",
    "model.add(TimeDistributed(\n",
    "    Conv2D(filters = 16, kernel_size = (2,2), activation='relu',data_format='channels_last', padding='same'), \n",
    "    input_shape=(365,8,21,3)))\n",
    "model.add(TimeDistributed(\n",
    "    Conv2D(filters = 16, kernel_size = (2,2), activation='relu',data_format='channels_last', padding='same'), \n",
    "    input_shape=(365,8,21,3)))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size = 2)))\n",
    "model.add(TimeDistributed(\n",
    "    Conv2D(filters = 32, kernel_size = (2,2), activation='relu',data_format='channels_last', padding='same'), \n",
    "    input_shape=(365,4,10,3)))\n",
    "model.add(TimeDistributed(\n",
    "    Conv2D(filters = 32, kernel_size = (2,2), activation='relu',data_format='channels_last', padding='same'), \n",
    "    input_shape=(365,4,10,3)))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size = 2)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "#LSTM model with time-distributed CNN as input\n",
    "model.add(LSTM(units = 40, activation='tanh', recurrent_activation='hard_sigmoid', \n",
    "               use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n",
    "               bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, \n",
    "               recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, \n",
    "               kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, implementation=1, \n",
    "               return_sequences=True, return_state=False))\n",
    "model.add(LSTM(units = 40, activation='tanh', recurrent_activation='hard_sigmoid', \n",
    "               use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n",
    "               bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, \n",
    "               recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, \n",
    "               kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, implementation=1, \n",
    "               return_sequences=False, return_state=False))\n",
    "model.add(Dense(194, activation = 'relu'))\n",
    "\n",
    "#compile\n",
    "print('Compiling model...')\n",
    "model.compile(loss=keras.losses.MSE,\n",
    "              optimizer=keras.optimizers.Adam(lr=0.01),\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 924,
     "status": "error",
     "timestamp": 1565221314552,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "Tlwf7WEZGa9U",
    "outputId": "598d3bc4-d592-4101-df54-b647488f9507"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-b3cd500cafaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m           validation_data=(x_test, y_test))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_1 to have shape (194,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "#fit\n",
    "batch_size = 64\n",
    "epochs = 3\n",
    "model.fit(x_train, y_train.T,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3THtfVQDGa9d"
   },
   "outputs": [],
   "source": [
    "print('Calculating predicted training values...')\n",
    "y_train_predicted = model.predict(x_train)\n",
    "\n",
    "print('Calculating predicted testing values...')\n",
    "y_test_predicted = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "szLRMC8bGa9h"
   },
   "outputs": [],
   "source": [
    "model.save('model_initial_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GCwgSec6Ga9j"
   },
   "outputs": [],
   "source": [
    "keras.models.load_model('model_initial_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mi4FNDObGa9m"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TjoFjzoGGa9q"
   },
   "outputs": [],
   "source": [
    "plt.plot(y_test_predicted[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test.T[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test.T[:,0],y_test_predicted[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"y_test_predicted\", y_test_predicted)\n",
    "np.savez(\"y_train_predicted\", y_train_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOM on seasonal hydrographs to see what our results are\n",
    "a = np.load('y_test_predicted.npz')\n",
    "y_test_predicted = a['arr_0']\n",
    "y_test = y_test_predicted.T\n",
    "\n",
    "y_new = np.empty((np.shape(y_test)[0],np.shape(y_test)[1]+1),dtype='single')\n",
    "y_new[:,1:] = y_test\n",
    "y_new[:,0] = y_new[:,1]\n",
    "\n",
    "y_new[:,1:] = y_test\n",
    "y_new[:,0] = y_new[:,1]\n",
    "\n",
    "plt.plot(y_new[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute smoothed seasonal over test period\n",
    "#plan: find leap years, remove last day of SF from leap years, reshape SF at each station into matrix, find average\n",
    "\n",
    "windowYears = flowDict['windowYears']\n",
    "trainYears = windowYears[trainInds]\n",
    "testYears = windowYears[testInds]\n",
    "\n",
    "#the first value in y_test is the second day in testInds\n",
    "y_new = np.empty((np.shape(y_test)[0],np.shape(y_test)[1]+1),dtype='single')\n",
    "y_new[:,1:] = y_test\n",
    "y_new[:,0] = y_new[:,1]\n",
    "\n",
    "lastDayOfYear = np.argwhere(testYears[1:] - testYears[:-1] == 1) + 1\n",
    "lastDayOfLeapYear = lastDayOfYear[np.argwhere(lastDayOfYear[1:]-lastDayOfYear[:-1] ==366)[:,0]+1]\n",
    "y_new1 = np.copy(y_new)\n",
    "y_new2 = np.delete(y_new1,lastDayOfLeapYear,axis=1)\n",
    "\n",
    "y_test_seasonal = np.empty((len(y_new2),365),dtype='single')\n",
    "y_test_seasonal_smooth = np.empty_like(y_test_seasonal)\n",
    "\n",
    "for streamGauge in range(len(y_new2)):\n",
    "    \n",
    "    dummy = np.reshape(y_new2[streamGauge,:],(10,365))\n",
    "    y_test_seasonal[streamGauge,:] = np.mean(dummy,axis=0)\n",
    "    \n",
    "    x = pd.Series(y_test_seasonal[streamGauge,:])      \n",
    "    y_test_seasonal_smooth[streamGauge] = x.rolling(30).mean()\n",
    "\n",
    "for ii in range(194):\n",
    "    plt.plot(y_test_seasonal_smooth[ii,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test_seasonal_smooth[0])\n",
    "\n",
    "from minisom import MiniSom   \n",
    "data = y_test_seasonal_smooth[:,31:-30]\n",
    "som = MiniSom(3, 2, 304, sigma=1, learning_rate=0.5) # initialization of 6x6 SOM\n",
    "som.pca_weights_init(data)\n",
    "som.train_random(data, 500) # trains the SOM with 100 iterations\n",
    "\n",
    "qnt = som.quantization(data)\n",
    "\n",
    "for ii in range(194):\n",
    "    plt.plot(qnt[ii,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = []\n",
    "bmus = []\n",
    "bmus_ref = [[1,4],[2,5],[3,6]]\n",
    "for ii in range(194):\n",
    "    w.append(som.winner(data[ii,:]))\n",
    "    bmus.append(bmus_ref[w[ii][0]][w[ii][1]])\n",
    "\n",
    "plot_AB()\n",
    "plt.scatter(stationLon,stationLat,c=bmus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Copy of main.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/andersonsam/cnn_lstm_era/blob/master/main.ipynb",
     "timestamp": 1565221436861
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
