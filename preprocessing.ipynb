{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing: ERA5, Flow, and Basin Outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import pickle\n",
    "from netCDF4 import Dataset\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "from pyproj import Proj, transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_betweenDates(initDate,finDate):\n",
    "    \n",
    "    \"\"\"\n",
    "    out: years, months, days, dayInds -- np arrays of the dates between (inclusive) initDate and finDate\n",
    "    in: initDate: [yyyy,mm,dd]\n",
    "        finDate: [yyyy,mm,dd]\n",
    "    \n",
    "    example:\n",
    "    \n",
    "    import numpy as np\n",
    "    initDate = [1979,1,1]\n",
    "    finDate = [2010,12,31]\n",
    "    years,months,days,dayInds = get_betweenDates(initDate,finDate)\n",
    "    \"\"\"\n",
    "\n",
    "    monthsInYear = np.hstack([1*np.ones((1,31)), 2*np.ones((1,28)), 3*np.ones((1,31)), 4*np.ones((1,30)), 5*np.ones((1,31)),\n",
    "                              6*np.ones((1,30)), 7*np.ones((1,31)), 8*np.ones((1,31)), 9*np.ones((1,30)), 10*np.ones((1,31)), \n",
    "                              11*np.ones((1,30)), 12*np.ones((1,31))])\n",
    "    monthsInYear = monthsInYear[0]\n",
    "    monthsInYear_ly = np.hstack([1*np.ones((1,31)), 2*np.ones((1,29)), 3*np.ones((1,31)), 4*np.ones((1,30)), 5*np.ones((1,31)), \n",
    "                                 6*np.ones((1,30)), 7*np.ones((1,31)), 8*np.ones((1,31)), 9*np.ones((1,30)), 10*np.ones((1,31)), \n",
    "                                 11*np.ones((1,30)), 12*np.ones((1,31))])\n",
    "    monthsInYear_ly = monthsInYear_ly[0]\n",
    "    daysInYear = np.hstack([range(1,32), range(1,29), range(1,32), range(1,31), range(1,32), range(1,31), range(1,32), \n",
    "                  range(1,32), range(1,31), range(1,32), range(1,31), range(1,32)])\n",
    "    daysInYear_ly = np.hstack([range(1,32), range(1,30), range(1,32), range(1,31), range(1,32), range(1,31), range(1,32), \n",
    "                  range(1,32), range(1,31), range(1,32), range(1,31), range(1,32)])\n",
    "\n",
    "    years = []\n",
    "    months = []\n",
    "    days = []\n",
    "    dayInds = []\n",
    "    for year in range(initDate[0],finDate[0]+1): #for each year, append on the right day/month/year vector\n",
    "\n",
    "        if np.mod(year,4)!=0: #if it is not a leap year\n",
    "            years.append(year*np.ones((1,365)))\n",
    "            months.append(monthsInYear)\n",
    "            days.append(daysInYear)\n",
    "            dayInds.append(range(1,366))\n",
    "        else: #if it is a leap year\n",
    "            years.append(year*np.ones((1,366)))\n",
    "            months.append(monthsInYear_ly)\n",
    "            days.append(daysInYear_ly)\n",
    "            dayInds.append(range(1,367))\n",
    "\n",
    "    years = np.hstack(years)[0]\n",
    "    months = np.hstack(months)\n",
    "    days = np.hstack(days)\n",
    "    dayInds = np.hstack(dayInds)\n",
    "    \n",
    "    return years, months, days, dayInds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature and Precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ProvinceDailyTemp(saveVars=0):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute daily temperature fields from ERA 5 dataset\n",
    "\n",
    "    example:\n",
    "    import geopandas as gpd\n",
    "    import numpy as np\n",
    "    from netCDF4 import Dataset\n",
    "    tempDict = get_ProvinceDailyTemp(saveVars=0)\n",
    "    \"\"\"\n",
    "\n",
    "    #open netcdf file\n",
    "    fileDirERA = './Data/ERA5/'\n",
    "\n",
    "    filenameERA = 'ERA5_T_1979_2015_6hourly_075_grid_AB_BC.nc'\n",
    "    filePathERA = fileDirERA + filenameERA\n",
    "    ERA = Dataset(filePathERA)\n",
    "\n",
    "    #extract data from file\n",
    "    lonERA = ERA.variables['longitude'][:] #longitude in degrees W \n",
    "    latERA = ERA.variables['latitude'][:]\n",
    "    hoursERA = ERA.variables['time'][:] #hours since Jan 1, 1900\n",
    "    T_hourly = ERA.variables['t2m'][:] #2-metre temperature, in Kelvin\n",
    "\n",
    "    #convert temp data at 00:00, 06:00, 12:00, 18:00 to daily averages\n",
    "    timestamps_per_day = 4\n",
    "    T = []\n",
    "    Tmax = []\n",
    "    Tmin = []\n",
    "    for daynum in range(int(len(hoursERA)/timestamps_per_day)): #for each day\n",
    "        inds = list(range(daynum*timestamps_per_day,daynum*timestamps_per_day+timestamps_per_day))\n",
    "        T.append(np.mean(T_hourly[inds,:,:],axis=0))\n",
    "        Tmax.append(np.max(T_hourly[inds,:,:],axis=0))\n",
    "        Tmin.append(np.min(T_hourly[inds,:,:],axis=0))\n",
    "\n",
    "    initDate = [1979,1,1]\n",
    "    finDate = [2015,12,31]\n",
    "    years,months,days,dayInds = get_betweenDates(initDate,finDate)\n",
    "\n",
    "    tempDict = {\n",
    "        'T':T,\n",
    "        'Tmax':Tmax,\n",
    "        'Tmin':Tmin,\n",
    "        'lonERA':lonERA,\n",
    "        'latERA':latERA,\n",
    "        'hoursERA':hoursERA,\n",
    "        'yearsERA':years,\n",
    "        'monthsERA':months,\n",
    "        'daysERA':days        \n",
    "    }\n",
    "\n",
    "    if saveVars:\n",
    "\n",
    "        pickle_filename = 'tempDict_ERA5_' + str(initDate[0]) + '_' + str(finDate[0]) + '_075grid_AB_BC.pickle'\n",
    "        pickle_out = open(pickle_filename,'wb')\n",
    "        pickle.dump(tempDict,pickle_out)\n",
    "        pickle_out.close()\n",
    "\n",
    "    return tempDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ProvinceDailyPrec(saveVars=0):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute daily total precipitation fields from ERA 5 dataset\n",
    "    \n",
    "    example:\n",
    "    import geopandas as gpd\n",
    "    import numpy as np\n",
    "    from netCDF4 import Dataset\n",
    "    precDict = get_ProvinceDailyPrec(saveVars=0)\n",
    "    \"\"\"\n",
    "    \n",
    "    #open netcdf file\n",
    "    fileDirERA = './Data/ERA5/'\n",
    "        \n",
    "    filenameERA = 'ERA5_P_1979_2015_6hourly_075_grid_AB_BC.nc'\n",
    "    filePathERA = fileDirERA + filenameERA\n",
    "    ERA = Dataset(filePathERA)\n",
    "\n",
    "    #extract data from file\n",
    "    lonERA = ERA.variables['longitude'][:] #longitude in degrees W \n",
    "    latERA = ERA.variables['latitude'][:]\n",
    "    hoursERA = ERA.variables['time'][:] #hours since Jan 1, 1900\n",
    "    P_6hourly = ERA.variables['tp'][:] #accumulated precipitation, in mm\n",
    "\n",
    "    #convert data at 00:00, 06:00, 12:00, 18:00 to daily totals\n",
    "    timestamps_per_day = 4\n",
    "    P = []\n",
    "    for daynum in range(int(len(hoursERA)/timestamps_per_day)): #for each day\n",
    "        inds = list(range(daynum*timestamps_per_day,daynum*timestamps_per_day+timestamps_per_day))\n",
    "        P.append(np.sum(P_6hourly[inds,:,:],axis=0))\n",
    "        \n",
    "    initDate = [1979,1,1]\n",
    "    finDate = [2015,12,31]\n",
    "    years,months,days,dayInds = get_betweenDates(initDate,finDate)\n",
    "        \n",
    "    precDict = {\n",
    "        'P':P,\n",
    "        'lonERA':lonERA,\n",
    "        'latERA':latERA,\n",
    "        'hoursERA':hoursERA,\n",
    "        'yearsERA':years,\n",
    "        'monthsERA':months,\n",
    "        'daysERA':days        \n",
    "    }\n",
    "        \n",
    "    if saveVars:\n",
    "        pickle_filename = 'precDict_ERA5_' + str(initDate[0]) + '_' + str(finDate[0]) + '_075grid_AB_BC.pickle'\n",
    "        pickle_out = open(pickle_filename,'wb')\n",
    "        pickle.dump(precDict,pickle_out)\n",
    "        pickle_out.close()   \n",
    "\n",
    "    return precDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDict = get_ProvinceDailyTemp(saveVars = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precDict = get_ProvinceDailyPrec(saveVars = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDict = get_ProvinceDailyTemp(saveVars = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.asarray(tempDict['T'])\n",
    "Tmax = np.asarray(tempDict['Tmax'])\n",
    "Tmin = np.asarray(tempDict['Tmin'])\n",
    "P = np.asarray(precDict['P'])\n",
    "\n",
    "eraLon = tempDict['lonERA']\n",
    "eraLat = tempDict['latERA']\n",
    "extentERA = [np.min(eraLon), np.max(eraLon), np.min(eraLat), np.max(eraLat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(np.mean(T,axis = 0), aspect = 'auto', cmap = 'RdBu', extent = extentERA)\n",
    "ax.set_title('Mean Temperature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(np.mean(P,axis = 0), aspect = 'auto', cmap = 'RdBu', extent = extentERA)\n",
    "ax.set_title('Mean Precipitation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ProvinceFlow(prov, yearRange, frac_missing_total_max, frac_missing_yearly_max, n_worse_years, computeFlow, saveFlowVars, params, verbose):\n",
    "\n",
    "    initDate = datetime(yearRange[0],1,1)\n",
    "    finDate = datetime(yearRange[1],12,31)\n",
    "    Nyears = len(range(yearRange[0], yearRange[1]+1))\n",
    "\n",
    "    folderWithFiles = './Data/Flow/' +prov #folder that contains all of the flow files\n",
    "\n",
    "    #get names of all files in folderWithFiles\n",
    "    flowFiles = []\n",
    "    for root, dirs, files in os.walk(folderWithFiles):\n",
    "        for name in sorted(files):\n",
    "            if name[0]=='0' or name[0]=='1': #if the current file is one that contains flow data (all flow files have a 0 or 1 at the start)\n",
    "                flowFiles.append(os.path.join(root,name)) #full path name of this file\n",
    "            else:\n",
    "                if name[0] != '.': #if it is not .DS_store, ie: if it is the table of data describing all files\n",
    "                    infofilename = os.path.join(root,name)\n",
    "\n",
    "        df = pd.read_csv(infofilename, encoding = 'ISO-8859-1') #read in summary data of all streamflow files\n",
    "        df.columns = [col.strip() for col in df.columns] #some columns have write space in the name -- remove this\n",
    "        df = df.drop([0]) #remove empty row\n",
    "        df.index = range(len(df)) #redo column indices (otherwise will start at 1 since we removed row 0)\n",
    "        totalStations = len(df) #the total number of stations present, including those that have missing data and/or wrong years\n",
    "        goodStations = [] #a list of indices of the stations which have sufficient data and the years of interest\n",
    "\n",
    "    dfs_flow_good = []\n",
    "\n",
    "    if computeFlow:\n",
    "\n",
    "    #####first, check to see if each station has enough data and the right years\n",
    "\n",
    "        yearmin = np.zeros([totalStations,1]) #minimum year with data in each station\n",
    "        yearmax = np.zeros_like(yearmin) #maximum year with data in each station\n",
    "\n",
    "        frac_missing_total_all = []\n",
    "        frac_missing_yearly_all = []\n",
    "\n",
    "        for station in range(totalStations): #for all stations (good and bad) that are active + naturalized flow in Alberta\n",
    "            \n",
    "            if verbose:\n",
    "                print('Checking station: ', station, '/', totalStations-1)\n",
    "\n",
    "            filename = flowFiles[station] #filename of current station\n",
    "            df_flow = pd.read_csv(filename) #create dataframe of all flow data for this station\n",
    "            df_flow = df_flow.drop([len(df_flow)-2,len(df_flow)-1]) #remove last two rows: not data, this is a disclaimer in each file\n",
    "\n",
    "            dates = df_flow['Date'] #these are the dates that are in the data -- missing dates are omitted and we will need to fill those eventually\n",
    "            dates_dt = [datetime.strptime(date,'%Y/%m/%d') for date in dates] #dates in datetime format for easier use\n",
    "            df_flow['Datetime'] = dates_dt #create datetime column in flow dataframe\n",
    "\n",
    "            df_flow.index = dates_dt #turn index from integers to datetime; can then fill missing data easily\n",
    "            df_flow = df_flow.reindex(pd.date_range(initDate,finDate),fill_value=np.nan) #missing dates are filled with nan; clip to be dates within yearRange\n",
    "\n",
    "            #calculate total fraction of missing days\n",
    "            frac_missing_total = np.sum(np.isnan(df_flow['Flow'])) / len(df_flow['Flow'])\n",
    "\n",
    "            #calculate fraction of missing days each year\n",
    "            frac_missing_yearly = [] #empty list -- append with fraction of missing dates in each year at this station\n",
    "            flow_mat = np.empty((Nyears, 365)) #rows are years, columns are days of that year (will ignore 366th day in leap years)\n",
    "            for year in range(Nyears): #for each year in yearRange\n",
    "                startDT = datetime(yearRange[0]+year, 1, 1) #starting date is January 1 of year\n",
    "                finDT = startDT + timedelta(days = 364) #ending date is 364 days later\n",
    "                flow_mat[year,:] = df_flow['Flow'][startDT:finDT] #row in flow_mat is the flow in this year\n",
    "                frac_missing_this_year = np.sum(np.isnan(flow_mat[year,:])) / 365 #number of missing dates this year divided by 365\n",
    "                frac_missing_yearly.append(frac_missing_this_year) #append fraction of missing dates\n",
    "\n",
    "            #consider it a good station if the total fraction of missing data is acceptable AND the missing fraction each year is acceptable\n",
    "            if frac_missing_total <= frac_missing_total_max and np.sort(frac_missing_yearly)[- n_worse_years - 1] <= frac_missing_yearly_max: #if the total fraction and yearly \n",
    "                goodStations.append(station)\n",
    "                dfs_flow_good.append(df_flow)\n",
    "\n",
    "            frac_missing_total_all.append(frac_missing_total)\n",
    "            frac_missing_yearly_all.append(frac_missing_yearly)\n",
    "\n",
    "    #####now, for each good station, compute flow/etc\n",
    "\n",
    "        N = len(goodStations) #number of good stations\n",
    "        all_flowseason = np.zeros([N,365])\n",
    "        all_flowseason_NF = np.zeros_like(all_flowseason)\n",
    "        all_flowseason_norm = np.zeros_like(all_flowseason)\n",
    "        all_flowseason_norm_NF = np.zeros_like(all_flowseason)\n",
    "        all_flowseason_norm_smooth = np.zeros_like(all_flowseason)\n",
    "        all_flowseason_norm_smooth_NF = np.zeros_like(all_flowseason)\n",
    "        yearvec = []\n",
    "        all_flow = []\n",
    "        all_flow_NF = []\n",
    "        all_flowwindow = []\n",
    "        all_flowwindow_NF = []\n",
    "        all_flowwindow_norm = []\n",
    "        all_flowwindow_norm_NF = []\n",
    "\n",
    "        for ind, station in enumerate(goodStations):\n",
    "            \n",
    "            if verbose:\n",
    "                print('Calculating good station: ', ind, '/', len(goodStations)-1)\n",
    "\n",
    "            df_flow = dfs_flow_good[ind]\n",
    "            flow_mat = np.empty((Nyears, 365))\n",
    "\n",
    "            for year in range(Nyears):\n",
    "                startDT = datetime(yearRange[0]+year, 1, 1)\n",
    "                finDT = startDT + timedelta(days = 364)\n",
    "                flow_mat[year,:] = df_flow['Flow'][startDT:finDT]\n",
    "\n",
    "            all_flowseason[ind] = np.squeeze(np.nanmean(flow_mat, axis = 0))\n",
    "            all_flowseason_NF[ind] = np.copy(all_flowseason[ind])\n",
    "            all_flowseason_NF[ind][np.isnan(all_flowseason[ind])] = np.nanmin(all_flowseason[ind])\n",
    "\n",
    "            flowseason_mean = np.mean(all_flowseason_NF[ind])\n",
    "            flowseason_std = np.std(all_flowseason_NF[ind])\n",
    "\n",
    "            all_flowseason_norm[ind] = (all_flowseason[ind] - flowseason_mean) / flowseason_std\n",
    "            all_flowseason_norm_NF[ind] = (all_flowseason_NF[ind] - flowseason_mean) / flowseason_std\n",
    "\n",
    "            all_flow.append(df_flow['Flow'])\n",
    "\n",
    "            flow_mat_NF = np.copy(flow_mat)\n",
    "            #flow_NF = np.empty((1,1))\n",
    "            for year in range(Nyears):\n",
    "                nan_inds = np.argwhere(np.isnan(flow_mat_NF[year,:]))\n",
    "                flow_mat_NF[year, nan_inds] = all_flowseason_NF[ind][nan_inds]\n",
    "                if year == 0:\n",
    "                    flow_NF = np.expand_dims(flow_mat_NF[year,:], axis = 1)\n",
    "                else:\n",
    "                    flow_NF = np.vstack((flow_NF, np.expand_dims(flow_mat_NF[year,:], axis = 1)))\n",
    "                if np.mod(yearRange[0] + year, 4)==0: #if leap year\n",
    "                    flow_NF = np.vstack((flow_NF, flow_NF[-1]))\n",
    "\n",
    "            all_flow_NF.append(flow_NF)\n",
    "\n",
    "    stationID = df['Station'].iloc[goodStations]\n",
    "    stationName = df['StationName'].iloc[goodStations]\n",
    "    stationLat = df['Latitude'].astype(float).iloc[goodStations]\n",
    "    stationLon = df['Longitude'].astype(float).iloc[goodStations]\n",
    "    stationDrainageArea = df['DrainageArea'].iloc[goodStations]\n",
    "\n",
    "    windowDatesTimestamp = pd.date_range(initDate,finDate)  \n",
    "    windowDates = [datetime.strftime(ii,'%Y-%m-%d') for ii in windowDatesTimestamp]\n",
    "    windowYears = np.asarray([int(d[0:4]) for d in windowDates])\n",
    "    windowMonths = np.asarray([int(d[5:7]) for d in windowDates])\n",
    "    windowDays = np.asarray([int(d[8:10]) for d in windowDates])\n",
    "\n",
    "    flowDict = {\n",
    "        'stationID':stationID,\n",
    "        'stationName':stationName,\n",
    "        'stationLat':stationLat,\n",
    "        'stationLon':stationLon,\n",
    "        'stationDrainageArea':stationDrainageArea,\n",
    "        'all_flowseason':all_flowseason,\n",
    "        'all_flowseason_NF':all_flowseason_NF,\n",
    "        'all_flowseason_norm':all_flowseason_norm,\n",
    "        'all_flowseason_norm_NF':all_flowseason_norm_NF,\n",
    "        #'all_flowseason_norm_smooth':all_flowseason_norm_smooth,\n",
    "        'all_flow':all_flow,\n",
    "        'all_flow_NF':all_flow_NF,\n",
    "        #'all_flowwindow':all_flowwindow,\n",
    "        #'all_flowwindow_NF':all_flowwindow_NF,\n",
    "        #'all_flowwindow_norm':all_flowwindow_norm,\n",
    "        #'all_flowwindow_norm_NF':all_flowwindow_norm_NF,\n",
    "        'windowDates':windowDates,\n",
    "        'windowYears':windowYears,\n",
    "        'windowMonths':windowMonths,\n",
    "        'windowDays':windowDays,\n",
    "        'params' : params\n",
    "    }\n",
    "\n",
    "    if saveFlowVars:\n",
    "        pickle_out = open(prov + '_flowvars_' + str(yearRange[0]) +  '_' + str(yearRange[1]) + '_missing_' + str(int(100*frac_missing_total_max)) + '_' + str(int(100*frac_missing_yearly_max)) + '_' + str(n_worse_years) + '.pickle','wb')\n",
    "        pickle.dump(flowDict,pickle_out)\n",
    "        pickle_out.close()\n",
    "        \n",
    "    return flowDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearRange = [1979, 2015]\n",
    "saveFlowVars = 0\n",
    "frac_missing_total_max = 0.4\n",
    "frac_missing_yearly_max = 0.4\n",
    "n_worse_years = 1 #number of years that can have greater than the 'frac_missing_yearly_max' fraction of missing data\n",
    "computeFlow = 1\n",
    "verbose = 0\n",
    "\n",
    "params = {'prov' : 'AB',\n",
    "          'yearRange' : yearRange,\n",
    "         'frac_missing_total_max' : frac_missing_total_max,\n",
    "         'frac_missing_yearly_max' : frac_missing_yearly_max,\n",
    "         'n_worse_years' : n_worse_years}\n",
    "\n",
    "flowAB = get_ProvinceFlow('AB', \n",
    "                          yearRange, \n",
    "                          frac_missing_total_max, \n",
    "                          frac_missing_yearly_max, \n",
    "                          n_worse_years, \n",
    "                          computeFlow, \n",
    "                          saveFlowVars, \n",
    "                          params,\n",
    "                          verbose)\n",
    "\n",
    "params = {'prov' : 'BC',\n",
    "          'yearRange' : yearRange,\n",
    "         'frac_missing_total_max' : frac_missing_total_max,\n",
    "         'frac_missing_yearly_max' : frac_missing_yearly_max,\n",
    "         'n_worse_years' : n_worse_years}\n",
    "\n",
    "flowBC = get_ProvinceFlow('BC', \n",
    "                          yearRange, \n",
    "                          frac_missing_total_max, \n",
    "                          frac_missing_yearly_max, \n",
    "                          n_worse_years, \n",
    "                          computeFlow, \n",
    "                          saveFlowVars, \n",
    "                          params, \n",
    "                          verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_flow_AB = np.asarray(flowAB['all_flow']) #all flow data from AB stations\n",
    "all_flow_NF_AB = np.asarray(flowAB['all_flow_NF']) #all flow data from AB stations, with nans filled (NF)\n",
    "all_flow_BC = np.asarray(flowBC['all_flow']) #all flow data from BC stations\n",
    "all_flow_NF_BC = np.asarray(flowBC['all_flow_NF']) #all flow data from BC stations, with nans filled (NF)\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 1, ncols = 1)\n",
    "ind = 0\n",
    "ax.plot(all_flow_NF_AB[ind][:365],'k--', linewidth = 1, label = 'NaN Filled')\n",
    "ax.plot(all_flow_AB[ind][:365], label = 'Original')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Day of Year')\n",
    "ax.set_ylabel('Streamflow [$m^3/s$]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basin Outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, need to load flow data and extract stationIDs to match with the Water Survey of Canada basin outlines\n",
    "\n",
    "dir_data = './Data'\n",
    "\n",
    "flowpickle = ['BC_flowvars_1979_2015_missing_40_40_1.pickle', 'AB_flowvars_1979_2015_missing_40_40_1.pickle'] #filenames of .pickle files which contain AB/BC streamflow data\n",
    "\n",
    "#open flow data; since flow data is provided at provincial level, loop through/open/concatenate data from desired provinces\n",
    "flowDicts = []\n",
    "for flowfile in flowpickle:\n",
    "  pickle_in = open(dir_data + '/Flow/' + flowfile,'rb')\n",
    "  flowDicts.append(pickle.load(pickle_in))\n",
    "\n",
    "#store flow data as a dictionary\n",
    "flowDict = {\n",
    "    'stationID' : np.hstack((flowDicts[0]['stationID'],flowDicts[1]['stationID'])), #station ID numbers\n",
    "    'stationLat' : np.hstack((flowDicts[0]['stationLat'],flowDicts[1]['stationLat'])), #latitude of each station, in degrees\n",
    "    'stationLon' : np.hstack((flowDicts[0]['stationLon'],flowDicts[1]['stationLon'])), #longitude of each station, in degrees\n",
    "}\n",
    "\n",
    "#unpack data\n",
    "stationLat = flowDict['stationLat']\n",
    "stationLon = flowDict['stationLon']\n",
    "stationID = flowDict['stationID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of unique filename prefixes in the WSC folder (the same filename prefix will have .gdbtable, .gdptablx, .spx, and .gdbindexes)\n",
    "\n",
    "dir_basins = './Data/WSC_Basins.gdb/'\n",
    "\n",
    "polygon_files = []\n",
    "unique_filenames = []\n",
    "\n",
    "for root, dirs, files in os.walk(dir_basins):\n",
    "    for name in sorted(files):\n",
    "        file = os.path.join(root,name)\n",
    "        file_short = file.replace('/','.').split('.')[-2]\n",
    "        polygon_files.append(file) #full path name of this file\n",
    "        if file_short[0] == 'a':\n",
    "            unique_filenames.append(file.replace('/','.').split('.')[-2])\n",
    "\n",
    "unique_filenames = list(set(unique_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there can be issues in identifying the CRS when reading the basin data using gpd.read_file\n",
    "#we will manually assign the CRS and define a transformation to lat/lon\n",
    "#the basin outline data is Canada_Albers_Equal_Area_Conic (ESRI:102001)\n",
    "#we will project it to latitude/longitude (EPSG:4326)\n",
    "\n",
    "inProj = Proj(init='esri:102001')\n",
    "outProj = Proj(init='epsg:4326')\n",
    "\n",
    "#these can be used to transform points x,y in Canada_Albers_Equal_Area_Conic to x1, y1 in lat/lon by: x1,y1 = transform(inProj, outProj, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationBasins = [None] * len(stationID)\n",
    "\n",
    "for zz, filename in enumerate(unique_filenames): #for each unique file prefix\n",
    "\n",
    "    #print statement every 100th file for keeping track\n",
    "    if np.mod(zz,100) == 0:\n",
    "        print(str(zz+1) + '/' + str(len(unique_filenames)))\n",
    "\n",
    "    #load one unique file\n",
    "    basin = gpd.read_file(root + filename + '.gdbtable') #read in basin info\n",
    "    \n",
    "    if len(basin.columns) == 8: #for properly formatted basins, there should be 8 columns (at least one basin is improperly formatted)\n",
    "\n",
    "        if basin.columns[0] != 'Station': #sometimes the first column is 'Station', sometimes 'STATION'; change all to 'Station' for indexing\n",
    "            curr_name = basin.columns[0] #'Station' or 'STATION'\n",
    "            basin = basin.rename(columns = {curr_name : 'Station'}) #change to 'Station'\n",
    "\n",
    "        if basin['Station'][0] in stationID: #if this current basin is a station in flow data\n",
    "\n",
    "            #reproject into lat/lon and overwrite the old projection polygon\n",
    "            x,y = basin['geometry'][0][0].exterior.xy\n",
    "            x1,y1 = transform(inProj, outProj, x, y)\n",
    "\n",
    "            kk = np.argwhere(basin['Station'][0] == stationID)[0][0]\n",
    "            stationBasins[kk] = Polygon([[x1[kk],y1[kk]] for kk in range(len(x1))]) #the basin of each stationID in flow data\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12,12))\n",
    "\n",
    "plot_prov_ax(prov = ['BC','AB'], ax = ax) \n",
    "\n",
    "for basin in stationBasins:\n",
    "    if basin is not None:\n",
    "        x,y = basin.exterior.xy\n",
    "        ax.plot(x, y, color = 'gray', zorder = 0, linewidth = 0.5)\n",
    "        ax.scatter(stationLon, stationLat, edgecolor = 'k', facecolor = 'w', s = 75, linewidth = 1, zorder = 3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveIt = 0\n",
    "\n",
    "yearRange = [1979,2015]\n",
    "\n",
    "if saveIt:\n",
    "    pickle_out = open('WSC_basins_' + str(yearRange[0]) +  '_' + str(yearRange[1]) + '_missing_' + str(int(100*frac_missing_total_max)) + '_' + str(int(100*frac_missing_yearly_max)) + '_' + str(n_worse_years) + '.pickle','wb')\n",
    "    pickle.dump(stationBasins,pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
