{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies for all functions\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pickle\n",
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_AB(prov='AB'):\n",
    "\n",
    "    \"\"\"\n",
    "    plot borders of alberta\n",
    "    \n",
    "    example:\n",
    "    import geopandas as gpd\n",
    "    import matplotlib.pyplot as plt\n",
    "    plot_AB()\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    \n",
    "    provIndex=0\n",
    "    provshapes_filename = '/Users/samanderson/Desktop/MATLAB/streamflow/Canada_Borders/PROVINCE.SHP'\n",
    "    provshapes = gpd.read_file(provshapes_filename)\n",
    "    provPoly = provshapes['geometry'][provIndex]\n",
    "    lonBorder,latBorder = provPoly.exterior.coords.xy \n",
    "\n",
    "    plt.plot(lonBorder,latBorder,'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rivers():\n",
    "\n",
    "    \"\"\"\n",
    "    plots rivers in Canada; takes a hot minute to run\n",
    "    \n",
    "    example:\n",
    "    import geopandas as gpd\n",
    "    import matplotlib.pyplot as plt\n",
    "    plot_rivers()\n",
    "    \"\"\"\n",
    "    \n",
    "    filename = '/Users/samanderson/Desktop/MATLAB/streamflow/Rivers_Lakes/RiversAndLakes_7.5m.shp'\n",
    "    rivshapes = gpd.read_file(filename)\n",
    "    rivPoly = rivshapes['geometry']\n",
    "\n",
    "    for ind in range(len(rivPoly)-1): #for each linestring object, plot water feature\n",
    "        lon,lat = rivPoly[ind].coords.xy \n",
    "        plt.plot(lon,lat,'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ProvinceDailyTemp(prov):\n",
    "    \n",
    "    \"\"\"\n",
    "    get T field in Alberta, from 1979 to 2010\n",
    "    \n",
    "    example:\n",
    "    import geopandas as gpd\n",
    "    import numpy as np\n",
    "    from netCDF4 import Dataset\n",
    "    T, lon, lat, hoursERA = get_ProvinceDailyTemp('AB')\n",
    "    \"\"\"\n",
    "\n",
    "    #get lat/lon of province\n",
    "    if prov=='AB':\n",
    "        provIndex = 0 #index in shapefile of province of interest\n",
    "    elif prov=='BC':\n",
    "        provIndex = 11\n",
    "    else:\n",
    "        print('ERROR: UNKNOWN PROVINCE')\n",
    "\n",
    "    provshapes_filename = '/Users/samanderson/Desktop/MATLAB/streamflow/Canada_Borders/PROVINCE.SHP'\n",
    "    provshapes = gpd.read_file(provshapes_filename)\n",
    "    provPoly = provshapes['geometry'][provIndex]\n",
    "    lonBorder,latBorder = provPoly.exterior.coords.xy \n",
    "\n",
    "    #open netcdf file\n",
    "    fileDirERA = '/Users/samanderson/Desktop/MATLAB/streamflow/'\n",
    "    filenameERA = 'interim_1979-01-01to2010-12-31_AB_BC.nc'\n",
    "    filePathERA = fileDirERA + filenameERA\n",
    "    ERA = Dataset(filePathERA)\n",
    "\n",
    "    #extract data from file\n",
    "    lonERA = ERA.variables['longitude'][:] #longitude in degrees W \n",
    "    lonERA = -np.abs(lonERA-360) #longitude, in -degrees W\n",
    "    latERA = ERA.variables['latitude'][:]\n",
    "    hoursERA = ERA.variables['time'][:] #hours since Jan 1, 1900\n",
    "    #hoursERA = hoursERA - hoursERA[0] #set first time to zero -- now measure time in hours from start of file\n",
    "    T_hourly = ERA.variables['t2m'][:] #2-metre temperature, in Kelvin\n",
    "\n",
    "    #convert temp data at 00:00, 06:00, 12:00, 18:00 to daily averages\n",
    "    T = []\n",
    "    for daynum in range(int(len(hoursERA)/4)): #for each day\n",
    "        inds = list(range(daynum*4,daynum*4+4))\n",
    "        T.append(np.mean(T_hourly[inds,:,:],axis=0))\n",
    "\n",
    "    #find which ERA lat/lon are within the province -- set others to nan\n",
    "    from descartes import PolygonPatch\n",
    "    borderPatch = PolygonPatch(provPoly)\n",
    "\n",
    "    inProv = np.zeros_like(T[0])\n",
    "    ilat = 0\n",
    "    ilon = 0\n",
    "    for latTest in latERA:\n",
    "        ilon=0\n",
    "        for lonTest in lonERA:\n",
    "            point = [lonTest,latTest]\n",
    "            inProv[ilat,ilon] = borderPatch.contains_point(point,radius=0)\n",
    "            if not inProv[ilat,ilon]:\n",
    "                inProv[ilat,ilon] = np.nan\n",
    "            ilon+=1\n",
    "        ilat+=1\n",
    "\n",
    "    initDate = [1979,1,1]\n",
    "    finDate = [2010,12,31]\n",
    "    years,months,days,dayInds = get_betweenDates(initDate,finDate)\n",
    "        \n",
    "    tempDict = {\n",
    "        'T':T,\n",
    "        'lonERA':lonERA,\n",
    "        'latERA':latERA,\n",
    "        'hoursERA':hoursERA,\n",
    "        'yearsERA':years,\n",
    "        'monthsERA':months,\n",
    "        'daysERA':days        \n",
    "    }\n",
    "\n",
    "    #for day in range(len(T)): #for each day, apply mask\n",
    "        #T[day]*=inProv\n",
    "\n",
    "    return tempDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ProvinceDailyPrec(prov):\n",
    "\n",
    "    \"\"\"\n",
    "    get P field in Alberta, from 1979 to 2010\n",
    "    \n",
    "    example:\n",
    "    import geopandas as gpd\n",
    "    import numpy as np\n",
    "    from netCDF4 import Dataset\n",
    "    P, lon, lat, hoursERA = get_ProvinceDailyPrec('AB')\n",
    "    \"\"\"\n",
    "    \n",
    "    #get lat/lon of province\n",
    "    if prov=='AB':\n",
    "        provIndex = 0 #index in shapefile of province of interest\n",
    "    elif prov=='BC':\n",
    "        provIndex = 11\n",
    "    else:\n",
    "        print('ERROR: UNKNOWN PROVINCE')\n",
    "\n",
    "    provshapes_filename = '/Users/samanderson/Desktop/MATLAB/streamflow/Canada_Borders/PROVINCE.SHP'\n",
    "    provshapes = gpd.read_file(provshapes_filename)\n",
    "    provPoly = provshapes['geometry'][provIndex]\n",
    "    lonBorder,latBorder = provPoly.exterior.coords.xy \n",
    "\n",
    "    #open netcdf file\n",
    "    fileDirERA = '/Users/samanderson/Desktop/MATLAB/streamflow/'\n",
    "    filenameERA = 'interim_1979-01-01to2010-12-31_AB_BC_Prec_12hr_step.nc'\n",
    "    filePathERA = fileDirERA + filenameERA\n",
    "    ERA = Dataset(filePathERA)\n",
    "\n",
    "    #extract data from file\n",
    "    lonERA = ERA.variables['longitude'][:] #longitude in degrees W \n",
    "    lonERA = -np.abs(lonERA-360) #longitude, in -degrees W\n",
    "    latERA = ERA.variables['latitude'][:]\n",
    "    hoursERA = ERA.variables['time'][:] #hours since Jan 1, 1900\n",
    "    #hoursERA = hoursERA - hoursERA[0] #set first time to zero -- now measure time in hours from start of file\n",
    "    P_12hourly = ERA.variables['tp'][:] #accumulated precipitation, in mm\n",
    "\n",
    "    #convert temp data at 00:00, 06:00, 12:00, 18:00 to daily averages\n",
    "    P = []\n",
    "    for daynum in range(int(len(hoursERA)/2)): #for each day\n",
    "        inds = list(range(daynum*2,daynum*2+2))\n",
    "        P.append(np.sum(P_12hourly[inds,:,:],axis=0))\n",
    "\n",
    "    #find which ERA lat/lon are within the province -- set others to nan\n",
    "    from descartes import PolygonPatch\n",
    "    borderPatch = PolygonPatch(provPoly)\n",
    "\n",
    "    inProv = np.zeros_like(P[0])\n",
    "    ilat = 0\n",
    "    ilon = 0\n",
    "    for latTest in latERA:\n",
    "        ilon=0\n",
    "        for lonTest in lonERA:\n",
    "            point = [lonTest,latTest]\n",
    "            inProv[ilat,ilon] = borderPatch.contains_point(point,radius=0)\n",
    "            if not inProv[ilat,ilon]:\n",
    "                inProv[ilat,ilon] = np.nan\n",
    "            ilon+=1\n",
    "        ilat+=1\n",
    "        \n",
    "    initDate = [1979,1,1]\n",
    "    finDate = [2010,12,31]\n",
    "    years,months,days,dayInds = get_betweenDates(initDate,finDate)\n",
    "        \n",
    "    precDict = {\n",
    "        'P':P,\n",
    "        'lonERA':lonERA,\n",
    "        'latERA':latERA,\n",
    "        'hoursERA':hoursERA,\n",
    "        'yearsERA':years,\n",
    "        'monthsERA':months,\n",
    "        'daysERA':days        \n",
    "    }\n",
    "\n",
    "    #for day in range(len(T)): #for each day, apply mask\n",
    "        #P[day]*=inProv\n",
    "\n",
    "    return precDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_betweenDates(initDate,finDate):\n",
    "    \n",
    "    \"\"\"\n",
    "    out: years, months, days, dayInds -- np arrays of the dates between (inclusive) initDate and finDate\n",
    "    in: initDate: [yyyy,mm,dd]\n",
    "        finDate: [yyyy,mm,dd]\n",
    "    \n",
    "    example:\n",
    "    \n",
    "    import numpy as np\n",
    "    initDate = [1979,1,1]\n",
    "    finDate = [2010,12,31]\n",
    "    years,months,days,dayInds = get_betweenDates(initDate,finDate)\n",
    "    \"\"\"\n",
    "\n",
    "    monthsInYear = np.hstack([1*np.ones((1,31)), 2*np.ones((1,28)), 3*np.ones((1,31)), 4*np.ones((1,30)), 5*np.ones((1,31)),\n",
    "                              6*np.ones((1,30)), 7*np.ones((1,31)), 8*np.ones((1,31)), 9*np.ones((1,30)), 10*np.ones((1,31)), \n",
    "                              11*np.ones((1,30)), 12*np.ones((1,31))])\n",
    "    monthsInYear = monthsInYear[0]\n",
    "    monthsInYear_ly = np.hstack([1*np.ones((1,31)), 2*np.ones((1,29)), 3*np.ones((1,31)), 4*np.ones((1,30)), 5*np.ones((1,31)), \n",
    "                                 6*np.ones((1,30)), 7*np.ones((1,31)), 8*np.ones((1,31)), 9*np.ones((1,30)), 10*np.ones((1,31)), \n",
    "                                 11*np.ones((1,30)), 12*np.ones((1,31))])\n",
    "    monthsInYear_ly = monthsInYear_ly[0]\n",
    "    daysInYear = np.hstack([range(1,32), range(1,29), range(1,32), range(1,31), range(1,32), range(1,31), range(1,32), \n",
    "                  range(1,32), range(1,31), range(1,32), range(1,31), range(1,32)])\n",
    "    daysInYear_ly = np.hstack([range(1,32), range(1,30), range(1,32), range(1,31), range(1,32), range(1,31), range(1,32), \n",
    "                  range(1,32), range(1,31), range(1,32), range(1,31), range(1,32)])\n",
    "\n",
    "    years = []\n",
    "    months = []\n",
    "    days = []\n",
    "    dayInds = []\n",
    "    for year in range(initDate[0],finDate[0]+1): #for each year, append on the right day/month/year vector\n",
    "\n",
    "        if np.mod(year,4)!=0: #if it is not a leap year\n",
    "            years.append(year*np.ones((1,365)))\n",
    "            months.append(monthsInYear)\n",
    "            days.append(daysInYear)\n",
    "            dayInds.append(range(1,366))\n",
    "        else: #if it is a leap year\n",
    "            years.append(year*np.ones((1,366)))\n",
    "            months.append(monthsInYear_ly)\n",
    "            days.append(daysInYear_ly)\n",
    "            dayInds.append(range(1,367))\n",
    "\n",
    "    years = np.hstack(years)[0]\n",
    "    months = np.hstack(months)\n",
    "    days = np.hstack(days)\n",
    "    dayInds = np.hstack(dayInds)\n",
    "    \n",
    "    return years, months, days, dayInds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ProvinceDailyStreamflow(prov='AB', yearType=3, computeFlow = 0, saveFlowVars = 0):\n",
    "    \n",
    "    \"\"\"\n",
    "    loads streamflow data for province of Alberta.\n",
    "    \n",
    "    prov: 'AB' ('BC' hopefully in future will work later)\n",
    "    yearType: 1==ActNat50; 2==ActNat40; 3==ActNat30; 4==ActReg40; 5==ActNatReg40; 6==ActLakes40; 7==ActNat30_2014\n",
    "    computeFlow: do you want to compute the flow from raw data (1) or do you want to just load saved data (0)?\n",
    "    saveFlowVars: if you are computing the flow, do you want to save the flow variables (1) or not (0)?\n",
    "    \n",
    "    example:\n",
    "    \n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from datetime import datetime\n",
    "    import numpy as np\n",
    "    import geopandas as gpd\n",
    "    import pickle\n",
    "    flowDict = get_ProvinceDailyStreamflow()\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #prov = 'AB'\n",
    "    #yearType = 3 #1==ActNat50; 2==ActNat40; 3==ActNat30; 4==ActReg40; 5==ActNatReg40; 6==ActLakes40; 7==ActNat30_2014\n",
    "    #computeFlow = 1 #1: open flow files and format variables; 0: open saved flow data\n",
    "    #saveFlowVars = 1 #1: save the flow variables; 0: don't save flow variables\n",
    "\n",
    "    folderWithFiles = '/Users/samanderson/Desktop/MATLAB/streamflow/ActNat30/ABActNat30' #folder that contains all of the flow files\n",
    "    yearRange = [1987,2010]\n",
    "    window = yearRange\n",
    "    windowyears = window[1] - window[0]\n",
    "\n",
    "    #get names of all files in 'folderWithFiles'\n",
    "    flowFiles = []\n",
    "    for root, dirs, files in os.walk(folderWithFiles):\n",
    "        for name in sorted(files):\n",
    "            if name[0]=='0' or name[0]=='1': #if the current file is one that contains flow data (all flow files have a 0 at the start)\n",
    "                flowFiles.append(os.path.join(root,name)) #full path name of this file\n",
    "            else: #if it is not a file that contains flow data\n",
    "                if name[0]!='.': #if it is not .DS_store, ie: if it is the 'ABActNat30.csv' (or equivalent) that is a table of data for each file\n",
    "                    infofilename = os.path.join(root,name)               \n",
    "\n",
    "        df = pd.read_csv(infofilename,encoding = \"ISO-8859-1\") #read in summary data of all streamflow files\n",
    "        cols = [col.strip() for col in df.columns] #some columns have white space in name -- remove this\n",
    "        df.columns = cols\n",
    "        df = df.drop([0,len(df)-2,len(df)-1]) #remove empty rows\n",
    "        df.index = range(len(df)) #redo column indices (otherwise will start at 1)\n",
    "        totalStations = len(df)\n",
    "        #totalStations = 3\n",
    "\n",
    "        #extract the most often used columns\n",
    "        stationID = df['Station']\n",
    "        stationName = df['StationName']\n",
    "        stationLat = df['Latitude'].astype(float)\n",
    "        stationLon = df['Longitude'].astype(float)\n",
    "        stationDrainageArea = df['DrainageArea']\n",
    "\n",
    "    if computeFlow:  \n",
    "\n",
    "        #initialize\n",
    "        all_flowseason = np.zeros([totalStations,365])\n",
    "        all_flowseason_NF = np.zeros_like(all_flowseason)\n",
    "        all_flowseason_norm = np.zeros_like(all_flowseason)\n",
    "        all_flowseason_norm_NF = np.zeros_like(all_flowseason)\n",
    "        all_flowseason_norm_smooth = np.zeros_like(all_flowseason)\n",
    "        all_flowseason_norm_smooth_NF = np.zeros_like(all_flowseason)\n",
    "        yearvec = []\n",
    "        yearmin = np.zeros([totalStations,1])\n",
    "        yearmax = np.zeros_like(yearmin)\n",
    "        all_flow = []\n",
    "        all_flow_NF = []\n",
    "        all_flowwindow = []\n",
    "        all_flowwindow_NF = []\n",
    "        all_flowwindow_norm = []\n",
    "        all_flowwindow_norm_NF = []\n",
    "\n",
    "        #get dates within window of interest\n",
    "        initDate = str(yearRange[0]) + '-01-01'\n",
    "        finDate = str(yearRange[1]) + '-12-31'\n",
    "        windowDatesTimestamp = pd.date_range(initDate,finDate)  \n",
    "        windowDates = [datetime.strftime(ii,'%Y-%m-%d') for ii in windowDatesTimestamp]\n",
    "        windowYears = np.asarray([int(d[0:4]) for d in windowDates])\n",
    "        windowMonths = np.asarray([int(d[5:7]) for d in windowDates])\n",
    "        windowDays = np.asarray([int(d[8:10]) for d in windowDates])\n",
    "\n",
    "        for ind in range(totalStations): #for each station/flowfile\n",
    "\n",
    "            print('Computing: Station ' + str(ind+1) + '/' + str(totalStations))\n",
    "\n",
    "            filename = flowFiles[ind]\n",
    "\n",
    "            df = pd.read_csv(filename)\n",
    "            df = df.drop([len(df)-2,len(df)-1])\n",
    "\n",
    "            dates = df['Date'] #these are the dates that are in the data -- missing dates are omitted -- want to fill\n",
    "\n",
    "            #reformat dates from yyyy/mm/dd to yyyy-mm-dd\n",
    "            objdates = [datetime.strptime(date,'%Y/%m/%d') for date in dates] \n",
    "            newdate = [datetime.strftime(date,'%Y-%m-%d') for date in objdates]\n",
    "            df.index = newdate\n",
    "\n",
    "            idx = pd.date_range(dates[0],dates[len(dates)-1]) #this is all of the dates\n",
    "            idxdate = [datetime.strftime(ii,'%Y-%m-%d') for ii in idx]\n",
    "            df = df.reindex(idxdate,fill_value=np.nan) #missing dates are filled with nan\n",
    "            df['Dates'] = idxdate #dates column is now filled\n",
    "\n",
    "            dates = df['Dates'] #filled with dates\n",
    "            flow = np.asarray(df['Flow']) #filled with nans\n",
    "            years = np.asarray([int(d[0:4]) for d in dates])\n",
    "            months = np.asarray([int(d[5:7]) for d in dates])\n",
    "            days = np.asarray([int(d[8:10]) for d in dates])\n",
    "            yearmin[ind] = np.min(years)\n",
    "            yearmax[ind] = np.max(years)\n",
    "\n",
    "            yearInds = [np.argwhere(years==year) for year in range(yearRange[0],yearRange[1]+1)]\n",
    "            currFlow = [[flow[ind] for ind in yearInd] for yearInd in yearInds]\n",
    "            #currFlow_NF = [currFlow[ind][np.isnan(all_flowseason[ind])]\n",
    "            currFlowMat = [currFlow[ind][:365] for ind in range(len(currFlow))]\n",
    "            currFlow_NF = []\n",
    "\n",
    "            all_flowseason[ind] = np.squeeze(np.nanmean(currFlowMat,0))\n",
    "\n",
    "            flowseason_mean = np.nanmean(all_flowseason[ind])\n",
    "            flowseason_std = np.nanstd(all_flowseason[ind])\n",
    "\n",
    "            all_flowseason[ind][np.isnan(all_flowseason[ind])] = np.nanmin(all_flowseason[ind])\n",
    "            all_flowseason_norm[ind] = (all_flowseason[ind] - flowseason_mean)/flowseason_std\n",
    "\n",
    "            all_flowwindow.append(np.vstack(currFlow))\n",
    "            all_flowwindow_norm.append((all_flowwindow[ind] - flowseason_mean)/flowseason_std)\n",
    "\n",
    "            x = pd.Series(all_flowseason_norm[ind])      \n",
    "            all_flowseason_norm_smooth[ind] = x.rolling(30).mean()\n",
    "\n",
    "            #fill nans: fill seasonal with min values; fill flow with seasonal\n",
    "            all_flowseason_NF[ind] = all_flowseason[ind]\n",
    "\n",
    "            all_flowseason_NF[ind][np.isnan(all_flowseason[ind])] = np.nanmin(all_flowseason[ind])\n",
    "            flowseason_mean_NF = np.nanmean(all_flowseason_NF[ind])\n",
    "            flowseason_std_NF = np.nanstd(all_flowseason_NF[ind])\n",
    "            all_flowseason_norm_NF[ind] = (all_flowseason_NF[ind] - flowseason_mean_NF)/flowseason_std_NF\n",
    "\n",
    "            x = pd.Series(all_flowseason_norm[ind])      \n",
    "            all_flowseason_norm_smooth_NF[ind] = x.rolling(30).mean()\n",
    "\n",
    "            #currFlow_NF = []\n",
    "            for ind1 in range(len(currFlow)):\n",
    "                dummy = np.squeeze(currFlow[ind1])\n",
    "                if len(np.argwhere(np.isnan(np.squeeze(np.squeeze(currFlow[ind1])))))>0: #if there are nans to fill\n",
    "                    if np.argwhere(np.isnan(np.squeeze(dummy)))[-1]<365: #if not a leap year\n",
    "                        dummy[np.argwhere(np.isnan(np.squeeze(dummy)))] = all_flowseason[ind][np.argwhere(np.isnan(np.squeeze(dummy)))]\n",
    "                        currFlow_NF.append(np.expand_dims(dummy,1))\n",
    "                    else: #if leap year\n",
    "                        dummy[np.argwhere(np.isnan(np.squeeze(dummy)))[0:-1]] = all_flowseason[ind][np.argwhere(np.isnan(np.squeeze(dummy)))[0:-1]]\n",
    "                        dummy[-1] = dummy[-2]\n",
    "                        currFlow_NF.append(np.expand_dims(dummy,1))\n",
    "                else: #if there are no nans to fill in the first place\n",
    "                    currFlow_NF = currFlow\n",
    "\n",
    "            currFlowMat_NF = [currFlow_NF[ind][:365] for ind in range(len(currFlow_NF))]\n",
    "\n",
    "            all_flowwindow_NF.append(np.vstack(currFlow_NF))\n",
    "            all_flowwindow_norm_NF.append((all_flowwindow_NF[ind] - flowseason_mean_NF)/flowseason_std_NF)\n",
    "\n",
    "            x = pd.Series(all_flowseason_norm_NF[ind])      \n",
    "            all_flowseason_norm_smooth_NF[ind] = x.rolling(30).mean()\n",
    "\n",
    "        flowDict = {\n",
    "            'stationID':stationID,\n",
    "            'stationName':stationName,\n",
    "            'stationLat':stationLat,\n",
    "            'stationLon':stationLon,\n",
    "            'stationDrainageArea':stationDrainageArea,\n",
    "            'all_flowseason':all_flowseason,\n",
    "            'all_flowseason_NF':all_flowseason_NF,\n",
    "            'all_flowseason_norm':all_flowseason_norm,\n",
    "            'all_flowseason_norm_NF':all_flowseason_norm_NF,\n",
    "            'all_flowseason_norm_smooth':all_flowseason_norm_smooth,\n",
    "            'all_flow':all_flow,\n",
    "            'all_flowwindow':all_flowwindow,\n",
    "            'all_flowwindow_NF':all_flowwindow_NF,\n",
    "            'all_flowwindow_norm':all_flowwindow_norm,\n",
    "            'all_flowwindow_norm_NF':all_flowwindow_norm_NF,\n",
    "            'windowDates':windowDates,\n",
    "            'windowYears':windowYears,\n",
    "            'windowMonths':windowMonths,\n",
    "            'windowDays':windowDays\n",
    "        }\n",
    "\n",
    "        if saveFlowVars:\n",
    "\n",
    "            pickle_out = open('ABActNat30_flowvars.pickle','wb')\n",
    "            pickle.dump(flowDict,pickle_out)\n",
    "            pickle_out.close()\n",
    "\n",
    "    else:\n",
    "\n",
    "        pickle_in = open('ABActNat30_flowvars.pickle','rb')\n",
    "        flowDict = pickle.load(pickle_in)\n",
    "\n",
    "        stationID = flowDict['stationID']\n",
    "        stationName = flowDict['stationName']\n",
    "        stationLat = flowDict['stationLat']\n",
    "        stationLon = flowDict['stationLon']\n",
    "        stationDrainageArea = flowDict['stationDrainageArea']\n",
    "        all_flowseason = flowDict['all_flowseason']\n",
    "        all_flowseason_norm = flowDict['all_flowseason_norm']\n",
    "        all_flow = flowDict['all_flow']\n",
    "        all_flowwindow = flowDict['all_flowwindow']\n",
    "        all_flowwindow_norm = flowDict['all_flowwindow_norm']\n",
    "        windowDates = flowDict['windowDates']\n",
    "        windowYears = flowDict['windowYears']\n",
    "        windowMonths = flowDict['windowMonths']\n",
    "        windowDays = flowDict['windowDays']\n",
    "            \n",
    "    return flowDict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
