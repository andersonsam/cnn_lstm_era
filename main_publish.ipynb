{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_publish.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM2AsEY/VA+DgerZlyJD8T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andersonsam/cnn_lstm_era/blob/master/main_publish.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZdU5-gl6rpd",
        "colab_type": "text"
      },
      "source": [
        "# Preliminary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBYj2_AS6t-8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "cee546c6-c815-4fd6-c8ea-5de3b62ff931"
      },
      "source": [
        "#download required libraries which are not in colab\n",
        "\n",
        "!pip install geopandas\n",
        "!pip install netCDF4\n",
        "!pip install minisom\n",
        "!pip install guppy3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting geopandas\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/a4/e66aafbefcbb717813bf3a355c8c4fc3ed04ea1dd7feb2920f2f4f868921/geopandas-0.8.1-py2.py3-none-any.whl (962kB)\n",
            "\u001b[K     |████████████████████████████████| 972kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: shapely in /usr/local/lib/python3.6/dist-packages (from geopandas) (1.7.1)\n",
            "Collecting fiona\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/8b/e8b2c11bed5373c8e98edb85ce891b09aa1f4210fd451d0fb3696b7695a2/Fiona-1.8.17-cp36-cp36m-manylinux1_x86_64.whl (14.8MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8MB 293kB/s \n",
            "\u001b[?25hCollecting pyproj>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/c3/071e080230ac4b6c64f1a2e2f9161c9737a2bc7b683d2c90b024825000c0/pyproj-2.6.1.post1-cp36-cp36m-manylinux2010_x86_64.whl (10.9MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9MB 39.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from geopandas) (1.0.5)\n",
            "Collecting cligj>=0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/e4/be/30a58b4b0733850280d01f8bd132591b4668ed5c7046761098d665ac2174/cligj-0.5.0-py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (1.15.0)\n",
            "Collecting click-plugins>=1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/da/824b92d9942f4e472702488857914bdd50f73021efea15b4cad9aca8ecef/click_plugins-1.1.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (7.1.2)\n",
            "Collecting munch\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (20.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->geopandas) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->geopandas) (1.18.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->geopandas) (2018.9)\n",
            "Installing collected packages: cligj, click-plugins, munch, fiona, pyproj, geopandas\n",
            "Successfully installed click-plugins-1.1.1 cligj-0.5.0 fiona-1.8.17 geopandas-0.8.1 munch-2.5.0 pyproj-2.6.1.post1\n",
            "Collecting netCDF4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/39/3687b2ba762a709cd97e48dfaf3ae36a78ae603ec3d1487f767ad58a7b2e/netCDF4-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.6/dist-packages (from netCDF4) (1.18.5)\n",
            "Collecting cftime\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/f4/31cb9b65f462ea960bd334c5466313cb7b8af792f272546b68b7868fccd4/cftime-1.2.1-cp36-cp36m-manylinux1_x86_64.whl (287kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 31.8MB/s \n",
            "\u001b[?25hInstalling collected packages: cftime, netCDF4\n",
            "Successfully installed cftime-1.2.1 netCDF4-1.5.4\n",
            "Collecting minisom\n",
            "  Downloading https://files.pythonhosted.org/packages/9d/10/a1c1621000d5ca00c41695689551c1a4d6d245d7bbf099d81e067da3e8f2/MiniSom-2.2.6.tar.gz\n",
            "Building wheels for collected packages: minisom\n",
            "  Building wheel for minisom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for minisom: filename=MiniSom-2.2.6-cp36-none-any.whl size=8525 sha256=2a1c2a450966e9d6e45096ce61ab04e7b8624ceb99861bea6947c2db4d9ccc45\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/c6/01/330066e36e1f7c826c96f656f9185822cfcdef0591315949ea\n",
            "Successfully built minisom\n",
            "Installing collected packages: minisom\n",
            "Successfully installed minisom-2.2.6\n",
            "Collecting guppy3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/de/482c672ba0fd3b2dfe95089104c2d328cd05459f01906ed17badca4caa39/guppy3-3.0.10.post1-cp36-cp36m-manylinux2010_x86_64.whl (597kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 8.4MB/s \n",
            "\u001b[?25hInstalling collected packages: guppy3\n",
            "Successfully installed guppy3-3.0.10.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Dq7H_5T7L5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load in functions required and dependencies to use them\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, TimeDistributed, LSTM, Dense, Dropout, GlobalMaxPooling2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm#, colors, path\n",
        "from mpl_toolkits.axes_grid.inset_locator import inset_axes, InsetPosition\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy import interpolate\n",
        "from scipy.stats import ks_2samp\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "from datetime import datetime, date\n",
        "from netCDF4 import Dataset\n",
        "from guppy import hpy\n",
        "from google.colab import drive\n",
        "\n",
        "from shapely.geometry import Point, Polygon\n",
        "from descartes import PolygonPatch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBXbqSFZAwj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#mount google drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm22ElpOAwuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define functions\n",
        "\n",
        "def nse(y_obs, y_model):\n",
        "\n",
        "  \"\"\"\n",
        "  NSE = nse(y_obs, y_model)\n",
        "\n",
        "  y_obs, y_model --> these are arrays of the same length (1 x N or N x 1) where N is the number of observations in time\n",
        "  \"\"\"\n",
        "\n",
        "  if not isinstance(y_obs, np.ndarray): #if tensor, convert to numpy array\n",
        "    y_obs = np.array(y_obs)\n",
        "  if not isinstance(y_model, np.ndarray):\n",
        "    y_model = np.array(y_model)\n",
        "\n",
        "  y_model = y_model.reshape((-1,1)) #make sure model and obs have same shape\n",
        "  y_obs = y_obs.reshape((-1,1))\n",
        "\n",
        "  nse = 1 - np.sum((y_model - y_obs)**2) / np.sum((y_obs - np.mean(y_obs))**2) #calculate NSE\n",
        "\n",
        "  return nse\n",
        "\n",
        "def plot_prov_ax(prov, ax):\n",
        "\n",
        "    \"\"\"\n",
        "    plot borders of a province on a given axis\n",
        "    \n",
        "    prov: string; 'AB', 'BC', or 'AB_BC'\n",
        "    ax: axis on which to plot the provincial borders\n",
        "    \"\"\"\n",
        "    \n",
        "    if prov == 'AB_BC':\n",
        "      provs = ['AB', 'BC']\n",
        "      for prov in provs:\n",
        "        if prov == 'AB':\n",
        "          provIndex=0\n",
        "        elif prov == 'BC':\n",
        "          provIndex = 11\n",
        "        provshapes_filename = '/content/drive/My Drive/Colab Notebooks/cnn_lstm_era/PROVINCE.SHP'\n",
        "        provshapes = gpd.read_file(provshapes_filename)\n",
        "        provPoly = provshapes['geometry'][provIndex]\n",
        "\n",
        "        if len(np.shape(provPoly)) == 0: #if only one polygon to plot\n",
        "\n",
        "          lonBorder,latBorder = provPoly.exterior.coords.xy \n",
        "          ax.plot(lonBorder,latBorder,'k')\n",
        "\n",
        "        else: #if multiply polygons in shape to plot\n",
        "\n",
        "          for ind in range(len(provPoly)):\n",
        "\n",
        "            lonBorder_segment,latBorder_segment = provPoly[ind].exterior.coords.xy \n",
        "            ax.plot(lonBorder_segment,latBorder_segment,'k')\n",
        "\n",
        "    else:\n",
        "      if prov == 'AB':\n",
        "        provIndex=0\n",
        "      elif prov == 'BC':\n",
        "        provIndex = 11\n",
        "      provshapes_filename = '/content/drive/My Drive/Colab Notebooks/cnn_lstm_era/PROVINCE.SHP'\n",
        "      provshapes = gpd.read_file(provshapes_filename)\n",
        "      provPoly = provshapes['geometry'][provIndex]\n",
        "\n",
        "      if len(np.shape(provPoly)) == 0: #if only one polygon to plot\n",
        "\n",
        "        lonBorder,latBorder = provPoly.exterior.coords.xy \n",
        "        ax.plot(lonBorder,latBorder,'k')\n",
        "\n",
        "      else: #if multiply polygons in shape to plot\n",
        "\n",
        "        for ind in range(len(provPoly)):\n",
        "\n",
        "          lonBorder_segment,latBorder_segment = provPoly[ind].exterior.coords.xy \n",
        "          ax.plot(lonBorder_segment,latBorder_segment,'k')\n",
        "\n",
        "def rmse(target,prediction):\n",
        "\n",
        "  \"\"\"\n",
        "  Returns the root-mean-square error between a target and prediction\n",
        "  target, prediction: arrays or tensors of equal length\n",
        "\n",
        "  Example: \n",
        "  RMSE = rmse(target,prediction) \n",
        "  \"\"\"\n",
        "\n",
        "  if not isinstance(target, np.ndarray):\n",
        "    target = np.array(target)\n",
        "  if not isinstance(prediction, np.ndarray):\n",
        "    prediction = np.array(prediction)\n",
        "\n",
        "  return(np.sqrt(((target.reshape(-1,1) - prediction.reshape(-1,1))**2).sum()/len(target.reshape(-1,1))))\n",
        "\n",
        "def get_A(heat):\n",
        "\n",
        "  \"\"\"\n",
        "  Returns the area (as a fraction of the total heatmap) which is above the half-maximum value\n",
        "\n",
        "  Example:\n",
        "  A = get_A(heat = heat_mean)\n",
        "  \"\"\"\n",
        "\n",
        "  halfMax = 0.5* (np.max(heat) - np.min(heat))\n",
        "  n_hot_pixels = len(np.argwhere((heat - np.min(heat)) > halfMax))\n",
        "  n_pixels = np.shape(heat)[0]*np.shape(heat)[1]\n",
        "  A = n_hot_pixels / n_pixels\n",
        "\n",
        "  return A\n",
        "\n",
        "def make_heat(model, x_test, y_test, style_dict, days, iters_total, iters_one_pass, output_dir, saveFiles, stationInds, verbose):\n",
        "\n",
        "  \"\"\"\n",
        "  model: \n",
        "      .h5 keras model\n",
        "  x_test:\n",
        "      tf tensor; test set of ERA data, input to model (shape = Ntest x 365 x height x width x channels)\n",
        "  y_test:\n",
        "      tf tensor; test set of streamflow data, target output of model (shape = Ntest x Nstations)\n",
        "  style_dict:\n",
        "      dictionary: {'style' : 'RISE' or 'gauss',\n",
        "                   'params' : [h,w,p_1] or sigma}\n",
        "          where [h,w,p_1] are the height/width/probability of perturbation of low-res mask (for RISE algorithm); sigma is the gaussian RMS width\n",
        "  days:\n",
        "      range of days in test set to perturb (e.g. days = range(0,365) will perturb the first 365 days in the test set)\n",
        "  iters_total:\n",
        "      number of total iterations of perturbation to do for each day in days\n",
        "  iters_one_pass:\n",
        "      number of iterations to do at one time (typically less than iters_total for memory limits)\n",
        "  output_dir:\n",
        "      directory where output will be saved, string \n",
        "  save_files:\n",
        "      0 or 1 (False or True) if output heat maps are to be saved to the output_dir or not\n",
        "  stationInds:\n",
        "      indices of stations corresponding to the output neurons of the keras model\n",
        "  verbose:\n",
        "      0: print nothing\n",
        "      1: print every 50th day\n",
        "      2: print every day and iteration\n",
        "  \"\"\"\n",
        "\n",
        "  n_channels = np.shape(x_test)[-1] #number of channels of input video\n",
        "  H = np.shape(x_test)[2] #height of input video, in pixels\n",
        "  W = np.shape(x_test)[3] #width of input video, in pixels\n",
        "\n",
        "  heat_all_slices = [[] for station in range(np.shape(y_test)[0])] #list of empty lists (one empty list per output station)\n",
        "  heat_days = [[] for station in range(np.shape(y_test)[0])] #list of empty lists (one empty list per output station)\n",
        "  jj = 0\n",
        "\n",
        "  for day in days: #for each day in test set that we will perturb\n",
        "\n",
        "    if verbose == 2 or (verbose == 1 and np.mod(day,50) == 0):\n",
        "      print('Day ' + str(day) + '/' + str(days[-1])) \n",
        "\n",
        "    for kk in range(int(iters_total/iters_one_pass)): #for each batch of iterations \n",
        "\n",
        "      if verbose == 2:\n",
        "        print('   Iteration: ' + str(kk*iters_one_pass) + '/' + str(iters_total))\n",
        "\n",
        "      iters = iters_one_pass \n",
        "\n",
        "      if style_dict['style'] == 'RISE':\n",
        "        \n",
        "        h = style_dict['params'][0]\n",
        "        w = style_dict['params'][1]\n",
        "        p_1 = style_dict['params'][2]\n",
        "\n",
        "        x_int = np.linspace(0,W,w) #low-res x indices\n",
        "        y_int = np.linspace(0,H,h) #low-res y indices\n",
        "\n",
        "        xnew = np.arange(W)\n",
        "        ynew = np.arange(H) \n",
        "\n",
        "        perturb_small = np.random.choice([0,1],size = (iters,1,h,w), p = [1-p_1,p_1])\n",
        "        perturb = np.half([interpolate.interp2d(x_int,y_int,perturb_small[iter][0])(xnew,ynew) for iter in range(iters)])\n",
        "\n",
        "      elif style_dict['style'] == 'gauss':\n",
        "        \n",
        "        sigma = style_dict['params']\n",
        "\n",
        "        x_int = np.arange(W)\n",
        "        y_int = np.arange(H)\n",
        "\n",
        "        x_mesh, y_mesh = np.meshgrid(x_int, y_int)\n",
        "        pointx = np.random.randint(0,np.shape(T[0])[1])\n",
        "        pointy = np.random.randint(0,np.shape(T[0])[0])\n",
        "        d2 = (x_mesh - pointx)**2 + (y_mesh - pointy)**2\n",
        "        perturb = np.half([np.exp( -d2 / (2*sigma**2)) for iter in range(iters)])\n",
        "\n",
        "      perturb_2D = np.copy(perturb)\n",
        "      perturb = tf.repeat(tf.expand_dims(tf.convert_to_tensor(perturb),3),nchannels, axis = 3)\n",
        "      perturb = tf.repeat(tf.expand_dims(tf.convert_to_tensor(perturb),1),365, axis = 1)\n",
        "\n",
        "      day_slice = [day]\n",
        "\n",
        "      xday = x_test[day]\n",
        "      xday_iters = [xday for val in range(iters)]\n",
        "\n",
        "      factor = np.random.choice([-1,1],p = [0.5,0.5]) #whether to add or subtract perturbation from input video\n",
        "      perturb = factor*perturb\n",
        "      x1 = perturb\n",
        "      x2 = tf.convert_to_tensor(xday_iters)\n",
        "      xday_iters_mask = tf.math.add(x1,x2)#.numpy()\n",
        "\n",
        "      x_all = tf.squeeze(tf.concat((tf.expand_dims(xday, axis = 0),xday_iters_mask), axis = 0))\n",
        "      x_all_ds = tf.data.Dataset.from_tensor_slices(x_all).batch(batch_size = 64)\n",
        "      y_all = model.predict(x_all_ds)\n",
        "\n",
        "      yday = y_all[:len(day_slice)]\n",
        "      yday_mask = y_all[len(day_slice):]\n",
        "\n",
        "      for station in range(np.shape(y_all)[1]):\n",
        "\n",
        "        yday_station = yday[:,station]\n",
        "        yday_station_mask = yday_mask[:,station]\n",
        "\n",
        "        ydiffs = np.abs(np.reshape([yday_station[jj] - yday_station_mask[jj*iters:jj*iters + iters] for jj in range(len(day_slice))],(-1,1)))\n",
        "        delta = np.ones((len(ydiffs),H,W)) * ydiffs[:,None]\n",
        "        heat_iters = [np.asarray(delta[jj*iters:(jj+1)*iters]) * np.asarray(perturb_2D) for jj in range(len(day_slice))]\n",
        "        heat_iters = np.reshape(heat_iters,(len(day_slice)*iters,H,W))\n",
        "        heat = [np.mean(heat_iters[jj*iters : (jj+1)*iters], axis=0) for jj in range(len(day_slice))] #fast\n",
        "        \n",
        "        heat_all_slices[station].append(heat[0]) #fast\n",
        "\n",
        "      del heat, heat_iters, delta, ydiffs, x_all, xday_iters\n",
        "\n",
        "    for station in range(np.shape(y_all)[1]):\n",
        "      heat_days[station].append(np.mean(heat_all_slices[station][jj : jj + int(iters_total/iters_one_pass)], axis = 0))\n",
        "    jj += int(iters_total/iters_one_pass)\n",
        "\n",
        "    heat_mean = np.empty( (np.shape(y_all)[1] ,) + np.shape(np.mean(heat_all_slices[0],axis=0)) )\n",
        "    for station in range(np.shape(y_all)[1]):\n",
        "      heat_mean[station] = np.mean(heat_all_slices[station],axis=0)\n",
        "\n",
        "  for zz, station in enumerate(stationInds):\n",
        "\n",
        "    heat_days_vec = np.empty((len(days),np.size(x_test[0,0,:,:,0])))\n",
        "    for day in days:\n",
        "      heat_days_vec[day,:] = np.reshape(heat_days[zz][day],(1,-1))\n",
        "\n",
        "    if saveFiles:\n",
        "\n",
        "      fileName = 'heat_mean_station_' + str(station) + '.csv'\n",
        "      fileName_days = 'heat_days_station_' + str(station) + '.csv'\n",
        "\n",
        "      if not path.exists(output_dir):\n",
        "        os.mkdir(output_dir)\n",
        "      np.savetxt(output_dir + '/' + fileName, heat_mean[zz], delimiter = ',')\n",
        "      #np.savetxt(output_dir + '/' + fileName_days, heat_days_vec, delimiter = ',')\n",
        "\n",
        "  return heat_mean, heat_days_vec\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYYGTNlYDnWq",
        "colab_type": "text"
      },
      "source": [
        "# Prep data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oozNS0wGAwwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prov = 'AB_BC' #for plotting (plot_prov)\n",
        "flowpickle = ['BC_flowvars_1979_2015.pickle', 'AB_flowvars_1979_2015.pickle'] #filenames of pickle files which contain AB/BC streamflow data\n",
        "basinspickle = 'AB_BC_basins2_1979_2015.pickle' #filename of pickle file which contains the basin outlines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V3EUspTTGa8s",
        "colab": {}
      },
      "source": [
        "#load data\n",
        "\n",
        "colabDataPath = '/content/drive/My Drive/Colab Notebooks/T_P_F_pca_lstm/'\n",
        "\n",
        "pickle_in = open(colabDataPath + basinspickle, 'rb')\n",
        "stationBasins = pickle.load(pickle_in)\n",
        "\n",
        "flowDicts = []\n",
        "for flowfile in flowpickle:\n",
        "  pickle_in = open(colabDataPath + flowfile,'rb')\n",
        "  flowDicts.append(pickle.load(pickle_in))\n",
        "\n",
        "flowDict = {\n",
        "    'stationID' : np.hstack((flowDicts[0]['stationID'],flowDicts[1]['stationID'])),\n",
        "    'stationName' : np.hstack((flowDicts[0]['stationName'],flowDicts[1]['stationName'])),\n",
        "    'stationLat' : np.hstack((flowDicts[0]['stationLat'],flowDicts[1]['stationLat'])),\n",
        "    'stationLon' : np.hstack((flowDicts[0]['stationLon'],flowDicts[1]['stationLon'])),\n",
        "    'stationDrainageArea' : np.hstack((flowDicts[0]['stationDrainageArea'],flowDicts[1]['stationDrainageArea'])),\n",
        "    'all_flowseason' : np.vstack((flowDicts[0]['all_flowseason'],flowDicts[1]['all_flowseason'])),\n",
        "    'all_flowseason_NF' : np.vstack((flowDicts[0]['all_flowseason_NF'],flowDicts[1]['all_flowseason_NF'])),\n",
        "    'all_flow' : np.vstack((flowDicts[0]['all_flow'],flowDicts[1]['all_flow'])),\n",
        "    'all_flow_NF' : np.vstack((flowDicts[0]['all_flow_NF'], flowDicts[1]['all_flow_NF'])),\n",
        "    'windowDates' : flowDicts[0]['windowDates'],\n",
        "    'windowYears' : flowDicts[0]['windowYears'],\n",
        "    'windowMonths' : flowDicts[0]['windowMonths'],\n",
        "    'windowDays' : flowDicts[0]['windowDays'],\n",
        "}\n",
        "\n",
        "pickle_in = open(colabDataPath + 'tempDict.pickle','rb')\n",
        "tempDict = pickle.load(pickle_in)\n",
        "\n",
        "pickle_in = open(colabDataPath + 'precDict.pickle','rb')\n",
        "precDict = pickle.load(pickle_in)\n",
        "\n",
        "pickle_in = open(colabDataPath + 'relHDict.pickle','rb')\n",
        "relHDict = pickle.load(pickle_in)\n",
        "\n",
        "pickle_in = open(colabDataPath + 'ssrdDict.pickle','rb')\n",
        "ssrdDict = pickle.load(pickle_in)\n",
        "\n",
        "#unpack data\n",
        "stationLat = flowDict['stationLat']\n",
        "stationLon = flowDict['stationLon']\n",
        "eraLat = tempDict['latERA']\n",
        "eraLon = tempDict['lonERA']\n",
        "\n",
        "flowDays = flowDict['windowDays']\n",
        "flowMonths = flowDict['windowMonths']\n",
        "flowYears = flowDict['windowYears']\n",
        "eraDays = tempDict['daysERA']\n",
        "eraMonths = tempDict['monthsERA']\n",
        "eraYears = tempDict['yearsERA']\n",
        "\n",
        "F = flowDict['all_flow_NF'] #discharge with nans filled (NF)\n",
        "Tmax = tempDict['Tmax']\n",
        "Tmin = tempDict['Tmin']\n",
        "P = precDict['P']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRsTfhfxTIsp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#select subset of stations\n",
        "\n",
        "maxLat = 56.\n",
        "stationInds = np.squeeze(np.argwhere(np.expand_dims(stationLat,1)<maxLat)[:,0])\n",
        "\n",
        "F = np.asarray(F)\n",
        "F = np.transpose(np.squeeze(F[stationInds]))\n",
        "\n",
        "stationBasins = [stationBasins[ii] for ii in stationInds]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw2t-indBi84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reduce spatial extent to only bound the stations of interest (reduce memory requirements)\n",
        "\n",
        "bounding_box = 1 #1/0 yes/no if you want to reduce the spatial extent\n",
        "border = 1 #number of pixels to border the outermost stations\n",
        "\n",
        "if bounding_box:\n",
        "\n",
        "  minLon = np.min(stationLon[stationInds])\n",
        "  maxLon = np.max(stationLon[stationInds])\n",
        "  minLat = np.min(stationLat[stationInds])\n",
        "  maxLat = np.max(stationLat[stationInds])\n",
        "\n",
        "  indMinLonERA = np.argmin(np.abs(eraLon - minLon))\n",
        "  indMaxLonERA = np.argmin(np.abs(eraLon - maxLon))\n",
        "  indMinLatERA = np.argmin(np.abs(eraLat - minLat))\n",
        "  indMaxLatERA = np.argmin(np.abs(eraLat - maxLat))\n",
        "\n",
        "  if indMinLatERA + border > len(eraLat) - 1:\n",
        "    indMinLatERA = len(eraLat) - 1\n",
        "  else:\n",
        "    indMinLatERA = indMinLatERA + border\n",
        "\n",
        "  if indMaxLatERA - border < 1:\n",
        "    indMaxLatERA = 0\n",
        "  else:\n",
        "    indMaxLatERA = indMaxLatERA - border + 1\n",
        "\n",
        "  if indMaxLonERA + border > len(eraLon) - 1:\n",
        "    indMaxLonERA = len(eraLon) - 1\n",
        "  else:\n",
        "    indMaxLonERA = indMaxLonERA + border\n",
        "\n",
        "  if indMinLonERA - border < 1:\n",
        "    indMinLonERA = 0\n",
        "  else:\n",
        "    indMinLonERA = indMinLonERA - border\n",
        "\n",
        "  Tmax = Tmax[:, indMaxLatERA:indMinLatERA+1, indMinLonERA:indMaxLonERA+1]\n",
        "  Tmin = Tmin[:, indMaxLatERA:indMinLatERA+1, indMinLonERA:indMaxLonERA+1]\n",
        "  P = P[:, indMaxLatERA:indMinLatERA+1, indMinLonERA:indMaxLonERA+1]\n",
        "\n",
        "  d_eraLon = eraLon[1] - eraLon[0]\n",
        "  d_eraLat = eraLat[0] - eraLat[1]\n",
        "\n",
        "  extentERA = [eraLon[indMinLonERA] - d_eraLon/2,eraLon[indMaxLonERA] + d_eraLon/2,eraLat[indMinLatERA] - d_eraLat/2,eraLat[indMaxLatERA] + d_eraLat/2]\n",
        "  eraLon = eraLon[indMinLonERA:indMaxLonERA+1]\n",
        "  eraLat = eraLat[indMaxLatERA:indMinLatERA+1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uAqKiacn7iIR",
        "colab": {}
      },
      "source": [
        "#standardize data\n",
        "\n",
        "#indices of testing/training\n",
        "trainStartYear = 1979\n",
        "trainFinYear = 1998\n",
        "valStartYear = 1999\n",
        "valFinYear = 2004\n",
        "testStartYear = 2005\n",
        "testFinYear = 2010\n",
        "\n",
        "trainInds = np.squeeze(np.argwhere((flowYears>=trainStartYear) & (flowYears<=trainFinYear)))\n",
        "valInds = np.squeeze(np.argwhere((flowYears>=valStartYear) & (flowYears<=valFinYear)))\n",
        "testInds = np.squeeze(np.argwhere((flowYears>=testStartYear) & (flowYears<=testFinYear)))\n",
        "Ntrain = len(trainInds)\n",
        "Nval = len(valInds)\n",
        "Ntest = len(testInds)\n",
        "\n",
        "#standardize weather variables (normalize wrt training period), then save as np.single for memory\n",
        "Tmaxmean_train = np.mean([Tmax[trainInds[ii]] for ii in range(len(trainInds))])\n",
        "Tmaxstd_train = np.std([Tmax[trainInds[ii]] for ii in range(len(trainInds))])\n",
        "Tmaxnorm = (Tmax - Tmaxmean_train)/Tmaxstd_train\n",
        "Tmaxnorm = np.single(Tmaxnorm)\n",
        "\n",
        "Tminmean_train = np.mean([Tmin[trainInds[ii]] for ii in range(len(trainInds))])\n",
        "Tminstd_train = np.std([Tmin[trainInds[ii]] for ii in range(len(trainInds))])\n",
        "Tminnorm = (Tmin - Tminmean_train)/Tminstd_train\n",
        "Tminnorm = np.single(Tminnorm)\n",
        "\n",
        "Pmean_train = np.mean([P[trainInds[ii]] for ii in range(len(trainInds))])\n",
        "Pstd_train = np.std([P[trainInds[ii]] for ii in range(len(trainInds))])\n",
        "Pnorm = (P - Pmean_train)/Pstd_train\n",
        "Pnorm = np.single(Pnorm)\n",
        "\n",
        "#normalize flow wrt to training period\n",
        "Fnorm = np.empty_like(F)\n",
        "for station in range(np.shape(F)[1]):\n",
        "    Fnorm[:,station] = (F[:,station] - np.mean(F[:,station]))/np.std(F[:,station])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4Dx0NakAw3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXIZ5FZLAw6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}