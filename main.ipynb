{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in functions required and dependencies to use them\n",
    "%run all_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get flow + met data\n",
    "flowDict = get_ProvinceDailyStreamflow()\n",
    "precDict = get_ProvinceDailyPrec('AB')\n",
    "tempDict = get_ProvinceDailyTemp('AB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unpack data\n",
    "\n",
    "stationLat = flowDict['stationLat']\n",
    "stationLon = flowDict['stationLon']\n",
    "eraLat = tempDict['latERA']\n",
    "eraLon = tempDict['lonERA']\n",
    "\n",
    "flowDays = flowDict['windowDays']\n",
    "flowMonths = flowDict['windowMonths']\n",
    "flowYears = flowDict['windowYears']\n",
    "eraDays = tempDict['daysERA']\n",
    "eraMonths = tempDict['monthsERA']\n",
    "eraYears = tempDict['yearsERA']\n",
    "\n",
    "F = flowDict['all_flowwindow_norm_NF'] #normalized discharge with nans filled (NF)\n",
    "T = tempDict['T']\n",
    "P = precDict['P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip to make data have same time range\n",
    "startYear = max(int(np.min(eraYears)),int(np.min(flowYears)))\n",
    "\n",
    "indStartERA = min(np.argwhere(eraYears==startYear))[0]\n",
    "indStartFlow = min(np.argwhere(flowYears==startYear))[0]\n",
    "\n",
    "F = F[indStartFlow:]\n",
    "T = T[indStartERA:]\n",
    "P = P[indStartERA:]\n",
    "\n",
    "flowDays = flowDays[indStartFlow:]\n",
    "flowMonths = flowMonths[indStartFlow:]\n",
    "flowYears = flowYears[indStartFlow:]\n",
    "\n",
    "eraDays = eraDays[indStartERA:]\n",
    "eraMonths = eraMonths[indStartERA:]\n",
    "eraYears = eraYears[indStartERA:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep data: make training and testing sets\n",
    "\n",
    "#indices of testing/training\n",
    "trainStartYear = 1987\n",
    "trainFinYear = 2000\n",
    "testStartYear = 2001\n",
    "testFinYear = 2010\n",
    "\n",
    "trainInds = np.squeeze(np.argwhere((flowYears>=trainStartYear) & (flowYears<=trainFinYear)))\n",
    "testInds = np.squeeze(np.argwhere((flowYears>testStartYear) & (flowYears<=testFinYear)))\n",
    "\n",
    "#input\n",
    "\n",
    "Tmean_train = np.mean([T[trainInds[ii]] for ii in range(len(trainInds))])\n",
    "Tstd_train = np.std([T[trainInds[ii]] for ii in range(len(trainInds))])\n",
    "Tnorm = (T - Tmean_train)/Tstd_train\n",
    "\n",
    "Pmean_train = np.mean([P[trainInds[ii]] for ii in range(len(trainInds))])\n",
    "Pstd_train = np.std([P[trainInds[ii]] for ii in range(len(trainInds))])\n",
    "Pnorm = (P - Pmean_train)/Pstd_train\n",
    "\n",
    "x_intermediate = np.zeros((8766,17,43,2))\n",
    "x_intermediate[:,:,:,0] = Tnorm\n",
    "x_intermediate[:,:,:,1] = Pnorm\n",
    "x_train_intermediate = x_intermediate[trainInds]\n",
    "x_test_intermediate = x_intermediate[testInds]\n",
    "\n",
    "#target\n",
    "y_train = np.squeeze([F[ii][trainInds[366:]] for ii in range(len(F))])\n",
    "y_test = np.squeeze([F[ii][testInds[366:]] for ii in range(len(F))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((8766-365,365,17,43,2))\n",
    "\n",
    "for ii in range(len(x_intermediate)-365):\n",
    "    x[ii] = x_intermediate[ii:ii+365]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x[trainInds]\n",
    "x_test = x[testInds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-191ea9c3d109>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((8766-365,365,17,43,2))\n",
    "x = [x_intermediate[ind:ind+365] for ind in range(len(x_intermediate)-365)]\n",
    "\n",
    "x_train = [x[ind] for ind in trainInds[:-366]]\n",
    "x_test = [x[ind] for ind in trainInds[:-366]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_array = np.asarray(x_train)\n",
    "x_test_array = np.asarray(x_test)\n",
    "\n",
    "#x_train = x[trainInds]\n",
    "#x_test = x[testInds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.asarray(x_train[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(np.squeeze(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194, 4748)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/samanderson/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "WARNING:tensorflow:From /Users/samanderson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Compiling model...\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import Model, Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, TimeDistributed, LSTM, Dense\n",
    "\n",
    "#CNN model\n",
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(\n",
    "    Conv2D(filters = 8, kernel_size = (3,3), activation='relu',data_format='channels_last'), \n",
    "    input_shape=(365,17,43,2)))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size = 2)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "#LSTM model with time-distributed CNN as input\n",
    "model.add(LSTM(units = 20, activation='tanh', recurrent_activation='hard_sigmoid', \n",
    "               use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n",
    "               bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, \n",
    "               recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, \n",
    "               kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, implementation=1, \n",
    "               return_sequences=False, return_state=False))\n",
    "model.add(Dense(194, activation = 'sigmoid'))\n",
    "\n",
    "#compile\n",
    "print('Compiling model...')\n",
    "model.compile(loss=keras.losses.MSE,\n",
    "              optimizer=keras.optimizers.Adam(lr=0.01),\n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 4748 arrays: [array([[[[-4.11131118e-01,  3.09574853e+00],\n         [-5.44301395e-01,  3.97064234e+00],\n         [-6.69991008e-01,  3.85533140e+00],\n         ...,\n         [-2.42223720e+00, -4.93264559e-01],\n     ...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-d62767f7e639>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m           validation_data=(x_test, y_test))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 4748 arrays: [array([[[[-4.11131118e-01,  3.09574853e+00],\n         [-5.44301395e-01,  3.97064234e+00],\n         [-6.69991008e-01,  3.85533140e+00],\n         ...,\n         [-2.42223720e+00, -4.93264559e-01],\n     ..."
     ]
    }
   ],
   "source": [
    "#fit\n",
    "batch_size = 64\n",
    "epochs = 3\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'asarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-d6122a234e04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'asarray'"
     ]
    }
   ],
   "source": [
    "type(x_train.asarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
