{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather --> CNN --> LSTM --> Streamflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3ba151f9a91f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#mount drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "#first: are you working in colab?\n",
    "colab = 1\n",
    "\n",
    "if colab:\n",
    "    \n",
    "    #mount drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    #define path to google drive data\n",
    "    dataPath = '/content/drive/My Drive/Colab Notebooks/T_P_F_pca_lstm/'\n",
    "\n",
    "    #download required libraries that are not already in colab\n",
    "    !pip install geopandas\n",
    "    \n",
    "else:\n",
    "    \n",
    "    dataPath = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from netCDF4 import Dataset\n",
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, LSTM, Flatten, TimeDistributed, Dropout, Input\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import Model, Sequential, regularizers\n",
    "import keras\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import interpolate\n",
    "import time\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions that we'll use\n",
    "\n",
    "def nse(y_obs, y_model):\n",
    "\n",
    "  \"\"\"\n",
    "  NSE = nse(y_obs, y_model)\n",
    "\n",
    "  y_obs, y_model --> these are arrays of the same length (1 x N or N x1) where N is the number of observations in time\n",
    "  \"\"\"\n",
    "\n",
    "  y_model = y_model.reshape((-1,1))\n",
    "  y_obs = y_obs.reshape((-1,1))\n",
    "  nse = 1 - np.sum((y_model - y_obs)**2) / np.sum((y_obs - np.mean(y_obs))**2)\n",
    "  return nse\n",
    "\n",
    "def nse_rolling(y_obs, y_model, window, stride = 1):\n",
    "\n",
    "  \"\"\"\n",
    "  NSE_rolling = nse_rolling(y_obs, y_model, window, stride)\n",
    "\n",
    "  y_obs, y_model --> these are arrays of the same length (1 x N or N x 1) where N is the number of observations in time\n",
    "  window --> this is the length of time over which to compute NSE, which will roll accross the total time period\n",
    "  stride --> default stride = 1; length of step to take when rolling (i.e. stride = 365 computes yearly NSE with no overlap)\n",
    "  \"\"\"\n",
    "\n",
    "  NSE_rolling = []\n",
    "\n",
    "  y_model = y_model.reshape((-1,1))\n",
    "  y_obs = y_obs.reshape((-1,1))\n",
    "\n",
    "  startInds = range(0, len(y_model) - window, stride)\n",
    "  for startInd in startInds:\n",
    "    y_model_window = y_model[startInd:startInd+window] \n",
    "    y_obs_window = y_obs[startInd:startInd+window]\n",
    "    NSE = nse(y_obs_window, y_model_window)\n",
    "    NSE_rolling.append(NSE)\n",
    "\n",
    "  return NSE_rolling\n",
    "\n",
    "def plot_AB(prov='AB'):\n",
    "\n",
    "    \"\"\"\n",
    "    plot borders of alberta\n",
    "    \n",
    "    example:\n",
    "    import geopandas as gpd\n",
    "    import matplotlib.pyplot as plt\n",
    "    plot_AB()\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    \n",
    "    provIndex=0\n",
    "    provshapes_filename = '/content/drive/My Drive/Colab Notebooks/cnn_lstm_era/PROVINCE.SHP'\n",
    "    provshapes = gpd.read_file(provshapes_filename)\n",
    "    provPoly = provshapes['geometry'][provIndex]\n",
    "    lonBorder,latBorder = provPoly.exterior.coords.xy \n",
    "\n",
    "    plt.plot(lonBorder,latBorder,'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "\n",
    "pickle_in = open(dataPath + 'flowDict.pickle','rb')\n",
    "flowDict = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(dataPath + 'tempDict_AB_ERA5.pickle','rb')\n",
    "tempDict = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(dataPath + 'precDict_AB_ERA5.pickle','rb')\n",
    "precDict = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(dataPath + 'spcHDict_AB_ERA5.pickle','rb')\n",
    "spcHDict = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(dataPath + 'ssrdDict_AB_ERA5.pickle','rb')\n",
    "ssrdDict = pickle.load(pickle_in)\n",
    "\n",
    "#unpack data\n",
    "\n",
    "stationLat = flowDict['stationLat']\n",
    "stationLon = flowDict['stationLon']\n",
    "eraLat = tempDict['latERA']\n",
    "eraLon = tempDict['lonERA']\n",
    "\n",
    "flowDays = flowDict['windowDays']\n",
    "flowMonths = flowDict['windowMonths']\n",
    "flowYears = flowDict['windowYears']\n",
    "eraDays = tempDict['daysERA']\n",
    "eraMonths = tempDict['monthsERA']\n",
    "eraYears = tempDict['yearsERA']\n",
    "\n",
    "F = flowDict['all_flowwindow_norm_NF'] #normalized discharge with nans filled (NF)\n",
    "T = tempDict['T']\n",
    "Tmax = tempDict['Tmax']\n",
    "Tmin = tempDict['Tmin']\n",
    "P = precDict['P']\n",
    "H = spcHDict['H']\n",
    "S = ssrdDict['S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set nan values to mean values of field (ie: out of province values\n",
    "\n",
    "meanT = np.nanmean(T)\n",
    "meanTmax = np.nanmean(Tmax)\n",
    "meanTmin = np.nanmean(Tmin)\n",
    "meanP = np.nanmean(P)\n",
    "meanH = np.nanmean(H)\n",
    "meanS = np.nanmean(S)\n",
    "\n",
    "Tall = np.copy(T)\n",
    "Tall[np.where(np.isnan(Tall))] = np.nanmean(T)\n",
    "T = Tall\n",
    "\n",
    "Tmaxall = np.copy(Tmax)\n",
    "Tmaxall[np.where(np.isnan(Tmaxall))] = np.nanmean(Tmax)\n",
    "Tmax = Tmaxall\n",
    "\n",
    "Tminall = np.copy(Tmin)\n",
    "Tminall[np.where(np.isnan(Tminall))] = np.nanmean(Tmin)\n",
    "Tmin = Tminall\n",
    "\n",
    "Pall = np.copy(P)\n",
    "Pall[np.where(np.isnan(Pall))] = np.nanmean(P)\n",
    "P = Pall\n",
    "\n",
    "Hall = np.copy(H)\n",
    "Hall[np.where(np.isnan(Hall))] = np.nanmean(H)\n",
    "H = Hall\n",
    "\n",
    "Sall = np.copy(S)\n",
    "Sall[np.where(np.isnan(Sall))] = np.nanmean(S)\n",
    "S = Sall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make data have same time range\n",
    "startYear = max(int(np.min(eraYears)),int(np.min(flowYears)))\n",
    "\n",
    "indStartERA = min(np.argwhere(eraYears==startYear))[0]\n",
    "indStartFlow = min(np.argwhere(flowYears==startYear))[0]\n",
    "\n",
    "F = np.asarray(np.transpose(np.squeeze(F[indStartFlow:])))\n",
    "T = np.asarray(T[indStartERA:])\n",
    "Tmax = np.asarray(Tmax[indStartERA:])\n",
    "Tmin = np.asarray(Tmin[indStartERA:])\n",
    "P = np.asarray(P[indStartERA:])\n",
    "H = np.asarray(H[indStartERA:])\n",
    "S = np.asarray(S[indStartERA:])\n",
    "\n",
    "##just alberta\n",
    "#T = T[:,:15,29:]\n",
    "#P = P[:,:15,29:]\n",
    "#H = H[:,:15,28:]\n",
    "#S = S[:,:15,28:]\n",
    "\n",
    "flowDays = flowDays[indStartFlow:]\n",
    "flowMonths = flowMonths[indStartFlow:]\n",
    "flowYears = flowYears[indStartFlow:]\n",
    "\n",
    "eraDays = eraDays[indStartERA:]\n",
    "eraMonths = eraMonths[indStartERA:]\n",
    "eraYears = eraYears[indStartERA:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(F),np.shape(T),np.shape(P),np.shape(H),np.shape(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep data: standardize\n",
    "\n",
    "#indices of testing/training\n",
    "trainStartYear = 1987\n",
    "trainFinYear = 2005\n",
    "testStartYear = 2006\n",
    "testFinYear = 2010\n",
    "\n",
    "trainInds = np.squeeze(np.argwhere((flowYears>=trainStartYear) & (flowYears<=trainFinYear)))\n",
    "testInds = np.squeeze(np.argwhere((flowYears>=testStartYear) & (flowYears<=testFinYear)))\n",
    "\n",
    "#standardize variables individually (normalize wrt training period), then save as 32-bit rather than 64-bit for space\n",
    "Tmean_train = np.mean([T[trainInds[ii]] for ii in range(len(trainInds))])\n",
    "Tstd_train = np.std([T[trainInds[ii]] for ii in range(len(trainInds))])\n",
    "Tnorm = (T - Tmean_train)/Tstd_train\n",
    "Tnorm = np.single(Tnorm)\n",
    "\n",
    "Tmaxmean_train = np.mean([Tmax[trainInds[ii]] for ii in range(len(trainInds))])\n",
    "Tmaxstd_train = np.std([Tmax[trainInds[ii]] for ii in range(len(trainInds))])\n",
    "Tmaxnorm = (Tmax - Tmaxmean_train)/Tmaxstd_train\n",
    "Tmaxnorm = np.single(Tmaxnorm)\n",
    "\n",
    "Tminmean_train = np.mean([Tmin[trainInds[ii]] for ii in range(len(trainInds))])\n",
    "Tminstd_train = np.std([Tmin[trainInds[ii]] for ii in range(len(trainInds))])\n",
    "Tminnorm = (Tmin - Tminmean_train)/Tminstd_train\n",
    "Tminnorm = np.single(Tnorm)\n",
    "\n",
    "Pmean_train = np.mean([P[trainInds[ii]] for ii in range(len(trainInds))])\n",
    "Pstd_train = np.std([P[trainInds[ii]] for ii in range(len(trainInds))])\n",
    "Pnorm = (P - Pmean_train)/Pstd_train\n",
    "Pnorm = np.single(Pnorm)\n",
    "\n",
    "Hmean_train = np.mean([H[trainInds[ii]] for ii in range(len(trainInds))])\n",
    "Hstd_train = np.std([H[trainInds[ii]] for ii in range(len(trainInds))])\n",
    "Hnorm = (H - Hmean_train)/Hstd_train\n",
    "Hnorm = np.single(Hnorm)\n",
    "\n",
    "Smean_train = np.mean([S[trainInds[ii]] for ii in range(len(trainInds))])\n",
    "Sstd_train = np.std([S[trainInds[ii]] for ii in range(len(trainInds))])\n",
    "Snorm = (S - Smean_train)/Sstd_train\n",
    "Snorm = np.single(Snorm)\n",
    "\n",
    "#Fmean_train = np.nanmean([F[ii][trainInds[366:]] for ii in range(len(F))])\n",
    "#Fstd_train = np.nanstd([F[ii][trainInds[366:]] for ii in range(len(F))])\n",
    "Fmean_train = np.mean(F[trainInds[366:],:])\n",
    "Fstd_train = np.std(F[trainInds[366:],:])\n",
    "Fnorm = (F - Fmean_train)/Fstd_train\n",
    "Fnorm = np.single(Fnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize flow\n",
    "\n",
    "for station in range(np.shape(F)[1]):\n",
    "    #F[:,station] = (F[:,station] - np.mean(F[:,station]))/np.std(F[:,station])\n",
    "    minF = np.min(F[:,station])\n",
    "    maxF = 2 * np.std(F[:,station])\n",
    "    F[:,station] = (F[:,station] - minF) / (maxF - minF)\n",
    "    \n",
    "#for inds in np.argwhere(F>10):\n",
    "#  F[inds[0],inds[1]] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct train and test predictor/target tensors\n",
    "\n",
    "#target data\n",
    "y_train = np.squeeze([Fnorm[366:len(trainInds)+366,ii] for ii in range(np.shape(F)[1])])\n",
    "y_test = np.squeeze([Fnorm[testInds[1:],ii] for ii in range(np.shape(F)[1])])\n",
    "\n",
    "#first, make (n_time x n_lon x n_lat x n_vars) tensor \n",
    "#x_intermediate = np.zeros((8766,17,43,2))\n",
    "x_intermediate = np.empty((8766,15,14,4),dtype='single')\n",
    "x_intermediate[:,:,:,0] = Tnorm\n",
    "x_intermediate[:,:,:,1] = Pnorm\n",
    "x_intermediate[:,:,:,2] = Hnorm\n",
    "x_intermediate[:,:,:,3] = Snorm\n",
    "x_train_intermediate = x_intermediate[trainInds]\n",
    "x_test_intermediate = x_intermediate[testInds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, convert x_intermediate into (n_time x 365 x n_lon x n_lat x n_vars) tensor\n",
    "x = np.empty((8766-365,365,15,14,4),dtype='single')\n",
    "x_train = np.empty((len(trainInds),365,15,14,4),dtype='single')\n",
    "x_test = np.empty((len(testInds)-1,365,15,14,4),dtype='single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JEECQyReGa86"
   },
   "outputs": [],
   "source": [
    "for ii in range(1000):\n",
    "    x_train[ii] = x_intermediate[ii:ii+365]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vC7SsmlDGa89"
   },
   "outputs": [],
   "source": [
    "for ii in range(1000,2000):\n",
    "    x_train[ii] = x_intermediate[ii:ii+365]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i2G5qpeU7iIf"
   },
   "outputs": [],
   "source": [
    "for ii in range(2000,3000):\n",
    "    x_train[ii] = x_intermediate[ii:ii+365]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T9gQXQAk7iIh"
   },
   "outputs": [],
   "source": [
    "for ii in range(3000,4000):\n",
    "    x_train[ii] = x_intermediate[ii:ii+365]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xYq9vC5d7iIj"
   },
   "outputs": [],
   "source": [
    "for ii in range(4000,len(trainInds)):\n",
    "    x_train[ii] = x_intermediate[ii:ii+365]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NPYCij1V7iIl"
   },
   "outputs": [],
   "source": [
    "for ii in range(1000):\n",
    "    x_test[ii] = x_intermediate[ii+len(trainInds)-365:ii+len(trainInds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "auRmmJ0c7iIn"
   },
   "outputs": [],
   "source": [
    "for ii in range(1000,2000):\n",
    "    x_test[ii] = x_intermediate[ii+len(trainInds)-365:ii+len(trainInds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Gr_E1lu7iIp"
   },
   "outputs": [],
   "source": [
    "for ii in range(2000,len(testInds)-1):\n",
    "    x_test[ii] = x_intermediate[ii+len(trainInds)-365:ii+len(trainInds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN model\n",
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(\n",
    "    Conv2D(filters = 8, kernel_size = (3,3), activation='relu',data_format='channels_last', padding='same'), \n",
    "    input_shape=(365,15,14,4)))\n",
    "model.add(TimeDistributed(\n",
    "    Conv2D(filters = 8, kernel_size = (3,3), activation='relu',data_format='channels_last', padding='same'), \n",
    "    input_shape=(365,15,14,4)))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size = 2)))\n",
    "model.add(TimeDistributed(\n",
    "    Conv2D(filters = 16, kernel_size = (2,2), activation='relu',data_format='channels_last', padding='same'), \n",
    "    ))\n",
    "model.add(TimeDistributed(\n",
    "    Conv2D(filters = 16, kernel_size = (2,2), activation='relu',data_format='channels_last', padding='same'), \n",
    "    ))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size = 2)))\n",
    "model.add(TimeDistributed(\n",
    "    Conv2D(filters = 32, kernel_size = (2,2), activation='relu',data_format='channels_last', padding='same'), \n",
    "    ))\n",
    "model.add(TimeDistributed(\n",
    "    Conv2D(filters = 32, kernel_size = (2,2), activation='relu',data_format='channels_last', padding='same'), \n",
    "    ))\n",
    "#model.add(TimeDistributed(MaxPooling2D(pool_size = 2)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "#LSTM model with time-distributed CNN as input\n",
    "#model.add(LSTM(units = 40, activation='tanh', recurrent_activation='hard_sigmoid', \n",
    "#               use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n",
    "#               bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, \n",
    "#               recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, \n",
    "#               kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, implementation=1, \n",
    "#               return_sequences=True, return_state=False))\n",
    "#model.add(LSTM(units = 40, activation='tanh', recurrent_activation='hard_sigmoid', \n",
    "#               use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n",
    "#               bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, \n",
    "#               recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, \n",
    "#               kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, implementation=1, \n",
    "#               return_sequences=False, return_state=False))\n",
    "#model.add(Dense(194, activation = 'relu'))\n",
    "\n",
    "model.add(LSTM(40, return_sequences=True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(LSTM(40, return_sequences=True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(LSTM(40, return_sequences=True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(194, activation = 'linear'))\n",
    "\n",
    "#compile\n",
    "print('Compiling model...')\n",
    "model.compile(loss=keras.losses.MSE,\n",
    "              optimizer=keras.optimizers.Adam(lr=0.005),\n",
    "              metrics=['mae'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', \n",
    "                   mode='min', \n",
    "                   verbose=1, \n",
    "                   patience = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "\n",
    "trainModel = 1\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 40\n",
    "\n",
    "if trainModel == 1:\n",
    "\n",
    "  history = model.fit(x_train,y_train, \n",
    "                      shuffle = True, \n",
    "                      epochs = epochs, \n",
    "                      batch_size = batch_size,\n",
    "                      verbose = 1, \n",
    "                      callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "\n",
    "saveModel = 0\n",
    "glacierStations = 0\n",
    "\n",
    "if saveModel == 1:\n",
    "\n",
    "  if glacierStations == 1:\n",
    "    modelName = str(num_stations) + '_stations_' + 'CNN_LSTM_DENSE_glacierStations_' + str(epochs) + '_epochs'\n",
    "    model.save(modelName + '.h5')\n",
    "  else:\n",
    "    modelName = str(num_stations) + '_stations_' + 'CNN_LSTM_DENSE_' + str(epochs) + '_epochs'\n",
    "    model.save(modelName + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "\n",
    "loadModel = 0\n",
    "\n",
    "if loadModel == 1:\n",
    "\n",
    "  dataPath = '/content/drive/My Drive/Colab Notebooks/cnn_lstm_era/'\n",
    "\n",
    "  if glacierStations == 1:\n",
    "    modelName = str(num_stations) + '_stations_' + 'CNN_LSTM_DENSE_glacierStations_' + str(epochs) + '_epochs'\n",
    "    model = load_model(dataPath + modelName + '.h5')\n",
    "  else:\n",
    "    modelName = str(num_stations) + '_stations_' + 'CNN_LSTM_DENSE_' + str(epochs) + '_epochs'\n",
    "    model = load_model(dataPath + modelName + '.h5')\n",
    "\n",
    "  #model = load_model(dataPath + str(num_stations) + '_stations_LSTM_DO_LSTM_DENSE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bulk = keras.models.clone_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss\n",
    "\n",
    "saveIt = 0\n",
    "\n",
    "if trainModel:\n",
    "\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "  epochs = range(1, len(loss) + 1)\n",
    "\n",
    "  plt.plot(loss, 'y', label='Training')\n",
    "  plt.plot(val_loss, 'r', label='Validation')\n",
    "  plt.title('Training and Validation Loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()\n",
    "  #plt.xlim((1,10))\n",
    "  #plt.ylim((0,.1))\n",
    "  #plt.show()\n",
    "\n",
    "  if saveIt:\n",
    "    plt.savefig('loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict streamflow with trained model\n",
    "\n",
    "y_testPredict = model.predict(x_test, batch_size = 8192, verbose = 1)\n",
    "y_predict = model.predict(x, batch_size=8192, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute NSE\n",
    "\n",
    "obs_per_station_test = int(len(y_test)/num_stations)\n",
    "window = 366\n",
    "NSE = nse(y_test,y_testPredict)\n",
    "NSE_rolling = nse_rolling(y_test, y_testPredict, window, stride = 365)\n",
    "NSE_station = [nse(y_test[kk*obs_per_station_test:(kk+1)*obs_per_station_test], y_testPredict[kk*obs_per_station_test:(kk+1)*obs_per_station_test]) for kk in range(num_stations)]\n",
    "\n",
    "print('Overall NSE = ' + str(NSE)[:4])\n",
    "print('Mean Station NSE = ' + str(np.mean(NSE_station))[:4])\n",
    "print('Median Station NSE = ' + str(np.median(NSE_station))[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize stations' performance\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "sns.distplot(NSE_station, bins = 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot model vs observed scatter plot\n",
    "\n",
    "saveIt = 0\n",
    "\n",
    "plt.figure(figsize = (8,8))\n",
    "\n",
    "plt.scatter(y_test, y_testPredict, alpha = 0.1)\n",
    "plt.xlabel('observation')\n",
    "plt.ylabel('model')\n",
    "plt.title('Model Results: NSE = ' + str(NSE)[:4])\n",
    "#plt.xlim((-2,5))\n",
    "#plt.xlim((0,4))\n",
    "#plt.yscale('log')\n",
    "#plt.xscale('log')\n",
    "\n",
    "if saveIt:\n",
    "  plt.savefig('obs_vs_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot time series of model and observations \n",
    "\n",
    "saveIt = 0\n",
    "\n",
    "plt.figure(figsize = (12,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(y_test, label = 'Observed')\n",
    "plt.plot(y_testPredict, label = 'Modelled')\n",
    "plt.legend()\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Normalized Streamflow')\n",
    "plt.title('Model Results: NSE = ' + str(NSE)[:4])\n",
    "plt.xlim((0,len(y_test)))\n",
    "plt.xlim((4*365*4,4*365*5))\n",
    "plt.ylim((0,3))\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "#plt.plot(range(int(window/2),len(y_test)-int(window/2)),NSE_rolling)\n",
    "plt.scatter(range(len(NSE_rolling)),NSE_rolling)\n",
    "plt.ylim((0,1))\n",
    "plt.xlim((0,len(NSE_rolling)))\n",
    "plt.xlim((15.5,19.5))\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('NSE')\n",
    "plt.title('Rolling NSE: Window = ' + str(window) + ' Days')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if saveIt:\n",
    "  plt.savefig('modelled_time_series.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cnn_lstm_era] *",
   "language": "python",
   "name": "conda-env-cnn_lstm_era-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
